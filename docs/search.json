[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"book introduction geographic data science using R, designed companion module GY7702 R Data Science MSc Geographic Information Science School Geography, Geology, Environment University Leicester. , much work progress.","code":""},{"path":"index.html","id":"aims-and-learning-objectives","chapter":"Welcome","heading":"Aims and learning objectives","text":"materials included book designed module focusing programming language R effective tool data science geographers. R one widely used programming languages, provides access vast repository programming libraries, cover aspects data science data wrangling statistical analysis, machine learning data visualisation. includes variety libraries processing spatial data, perform geographic information analysis, create maps. , R extremely versatile, free opensource tool geographic information science, combines capabilities traditional GIS software advantages scripting language, interface vast array algorithms.materials aim cover necessary skills basic programming, data wrangling reproducible research tackle sophisticated non-spatial data analyses. first part module focus core programming techniques, data wrangling practices reproducible research. second part module focus non-spatial data analysis approaches, including statistical analysis machine learning.book lecture slides use #FFF0E2 background colour avoid use pure white background, can make reading difficult slower people dyslexia. colours also checked readability using Colour Contrast Analyser.","code":""},{"path":"index.html","id":"license-and-acknowledgements","chapter":"Welcome","heading":"License and acknowledgements","text":"work licensed GNU General Public License v3.0 except specified. text licensed Creative Commons Attribution-ShareAlike 4.0 International (CC -SA 4.0). See src/images foler information regarding images used materials. Contains public sector information licensed Open Government Licence v3.0 information derived data sources Office National Statistics, Ministry Housing, Communities & Local Government, Ofcom, institutions UK Government Open Government License v3 (see data folder information).repository includes teaching materials created (Dr Stefano De Sabbata) module GY7702 R Data Science, working School Geography, Geology, Environment University Leicester. also like acknowledge contributions made parts materials Prof Chris Brunsdon Prof Lex Comber (see also Introduction R Spatial Analysis Mapping, Sage, 2015), Dr Marc Padilla, Dr Nick Tate, convened previous versions module University Leicester.Last least, like acknowledge miriad small contributions users many plaftforms, including Stack Exchange Network (e.g., Stack Overflow, provided Creative Commons Attribution-Share Alike 2.5 Generic License), asked answered many questions coding book. impossible trace back contributors pages, scripts years, . learning materials created using R, RStudio, RMarkdown Bookdown (many thanks Yihui Xie fantastic tools related documentation), GitHub.","code":""},{"path":"index.html","id":"about-me","chapter":"Welcome","heading":"About me","text":"Hi , name Stef Lecturer Quantitative Geography School Geography, Geology Environment University Leicester Research associate Oxford Internet Institute University Oxford.geographic data scientist working intersection human geography, artificial intelligence internet studies. research focuses three intertwined research streams: development machine learning approaches geographic data analysis, study geographies content created internet platforms, application quantitative urban geography. teach data science programming R, information visualization, geospatial databases information retrieval, digital geographies geographic information science. secretary Geographic Information Science Research Group Royal Geographical Society IBG. also\npart steering committee GIScience Research UK (GISRUK) chair GISRUK 2018 conference. member Commission Location Based Services International Cartographic Association.Earlier career, worked Researcher Oxford Internet Institute University Oxford (2013-2015) Junior Research Fellow Wolfson College University Oxford (2014-2015). awarded PhD Department Geography University Zurich 2013, BSc MSc computer science Department Mathematics Computer Science University Udine.can find Twitter [@maps4thought](https://twitter.com/maps4thought) GitHub sdesabbata.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"lecture-slides","chapter":"Preface","heading":"Lecture slides","text":"R coding\n100 Introduction\n101 Introduction R\n102 Core concepts\n103 Tidyverse\n\n110 R programming\n111 Data types (vectors, factors, matrices, arrays, lists)\n112 Control structures (conditional statements, loops)\n113 Functions\n\n100 Introduction\n101 Introduction R\n102 Core concepts\n103 Tidyverse\n101 Introduction R102 Core concepts103 Tidyverse110 R programming\n111 Data types (vectors, factors, matrices, arrays, lists)\n112 Control structures (conditional statements, loops)\n113 Functions\n111 Data types (vectors, factors, matrices, arrays, lists)112 Control structures (conditional statements, loops)113 FunctionsData wrangling\n200 Selection manipulation\n201 Data Frames\n202 Data selection filtering\n203 Data manipulation\n\n210 Table operations\n211 Join operations\n212 Table pivot\n213 Read write data\n\n220 Reproducibility\n221 Reproducibility\n222 R Markdown\n223 Git\n\n200 Selection manipulation\n201 Data Frames\n202 Data selection filtering\n203 Data manipulation\n201 Data Frames202 Data selection filtering203 Data manipulation210 Table operations\n211 Join operations\n212 Table pivot\n213 Read write data\n211 Join operations212 Table pivot213 Read write data220 Reproducibility\n221 Reproducibility\n222 R Markdown\n223 Git\n221 Reproducibility222 R Markdown223 GitData analysis\n300 Exploratory data analysis\n301 Data visualisation\n302 Descriptive statistics\n303 Exploring assumptions\n\n310 Comparing data\n311 Comparing groups\n312 Correlation\n313 Data transformations\n\n320 Regression models\n321 Simple regression\n322 Assessing regression assumptions\n323 Multiple regression\n\n300 Exploratory data analysis\n301 Data visualisation\n302 Descriptive statistics\n303 Exploring assumptions\n301 Data visualisation302 Descriptive statistics303 Exploring assumptions310 Comparing data\n311 Comparing groups\n312 Correlation\n313 Data transformations\n311 Comparing groups312 Correlation313 Data transformations320 Regression models\n321 Simple regression\n322 Assessing regression assumptions\n323 Multiple regression\n321 Simple regression322 Assessing regression assumptions323 Multiple regressionMachine learning\n400 Supervised\n401 Introduction Machine Learning\n402 Artificial Neural Networks\n403 Support vector machines\n\n410 Unsupervised\n411 Principal Component Analysis\n412 Centroid-based clustering\n413 Hierarchical density-based clustering\n\n400 Supervised\n401 Introduction Machine Learning\n402 Artificial Neural Networks\n403 Support vector machines\n401 Introduction Machine Learning402 Artificial Neural Networks403 Support vector machines410 Unsupervised\n411 Principal Component Analysis\n412 Centroid-based clustering\n413 Hierarchical density-based clustering\n411 Principal Component Analysis412 Centroid-based clustering413 Hierarchical density-based clustering","code":""},{"path":"preface.html","id":"reference-books","chapter":"Preface","heading":"Reference books","text":"Suggested readingR Data Science Garrett Grolemund Hadley Wickham, O’Reilly Media, 2016. See online book.Machine Learning R: Expert techniques predictive modeling Brett Lantz, Packt Publishing, 2019. See book webpage.readingProgramming Skills Data Science: Start Writing Code Wrangle, Analyze, Visualize Data R Michael Freeman Joel Ross, Addison-Wesley, 2019. See book webpage repository.Art R Programming: Tour Statistical Software Design Norman Matloff, Starch Press, 2011. See book webpage.Discovering Statistics Using R Andy Field, Jeremy Miles Zoë Field, SAGE Publications Ltd, 2012. See book webpage.Introduction Statistical Learning Applications R Gareth James, Daniela Witten, Trevor Hastie Robert Tibshirani, Springer, 2013. See book webpage.Introduction Machine Learning R Scott V. Burger, O’Reilly Media, 2018. See book webpage.Machine Learning R, tidyverse, mlr Hefin . Rhys, Manning Publications, 2020. See book webpage.Deep Learning R François Chollet J. J. Allaire, Manning Publications, 2018. See book webpage.Introduction R Spatial Analysis Mapping Chris Brunsdon Lex Comber, Sage, 2015. See book webpage.Geocomputation R Robin Lovelace, Jakub Nowosad Jannes MuenchowSee, CRC Press, 2019. See online book.","code":""},{"path":"preface.html","id":"reproducibility","chapter":"Preface","heading":"Reproducibility","text":"","code":""},{"path":"preface.html","id":"instructor","chapter":"Preface","heading":"Instructor","text":"can now reproduce granolarr using Docker. First install Docker system, install Git already installed, clone repository GitHub. can either build sdesabbata/granolarr image running Docker_Build.sh script root directory repository simply pull latest sdesabbata/granolarr image Docker Hub.now code computational environment reproduce materials, can done running script Docker_Make.sh (Docker_Make_WinPowerShell.sh Windows using PowerShell) repository folder. script instantiate Docker container sdesabbata/granolarr image, bind mount repository folder container execute Make.R container, clearing re-making materials. data used materials can re-created original open data using scripts src/utils, described data/README.md.instance, unix-based system like Linux Mac OS, can reproduce granolarr using following four commands:approach allow simply use materials , easily edit create version computational environment. develop materials, simply modify code repository run Docker_Make.sh repository folder obtain updated materials.RMarkdown code used create materials lectures practical sessions can found src/lectures src/practicals folders, respectively. folders contain one RMarkdown file per session contains headings necessary create respective html slides (compiled docs/lectures/html) pdf documents (compiled docs/practicals/pdf), whereas main corpus materials can found files included respective contents folders. latter files also used directly generate Bookdown version materials (compiled docs/lectures/bookdown docs/practicals/bookdown). docs folder also contains files used generate GitHub Pages website using Minimal Mistakes Jekyll theme. utils folder also contains IOSlides templates style classes used RMarkdown code.can edit materials granolarr repository folder using RStudio another editor computer compile new materials using Docker. Alternatively, can follow learner instructions start RStudio Server using Docker, develop materials environment compiled. first option might quicker minor edits, whereas latter option might preferable substantial modifications, especially might need test code.","code":"docker pull sdesabbata/granolarr:latest\ngit clone https://github.com/sdesabbata/granolarr.git\ncd granolarr\n./Docker_Make.sh.\n├── DockerConfig\n├── data\n├── docs\n│   ├── _data\n│   ├── _pages\n│   ├── _posts\n│   ├── assets\n│   │   └── images\n│   ├── exercises\n│   ├── lectures\n│   │   ├── bookdown\n│   │   └── html\n│   └── practicals\n│       ├── bookdown\n│       └── pdf\n└── src\n    ├── lectures\n    │   ├── contents\n    │   └── images\n    ├── practicals\n    │   ├── contents\n    │   ├── images\n    │   └── materials\n    └── utils\n        ├── IOSlides\n        └── RMarkdown"},{"path":"preface.html","id":"learner","chapter":"Preface","heading":"Learner","text":"learner, can use Docker follow practical sessions instructions complete exercises. First install Docker system, install Git already installed, clone repository GitHub.can either build sdesabbata/granolarr image running Docker_Build.sh script root directory repository simply pull latest sdesabbata/granolarr image Docker Hub.now code computational environment reproduce materials, can done running script Docker_RStudio_Start.sh (Docker_RStudio_Start_WinPowerShell.sh Windows using PowerShell) repository folder.instance, unix-based system like Linux Mac OS, can set start granolarr container using following four commands:Docker_RStudio_Start.sh script first create my_granolarr folder parent directory root directory repository (doesn’t exitst). script instantiate Docker container sdesabbata/granolarr image, bind mount my_granolarr folder granolarr repository folder container start RStudio Server.Using browser, can access RStudio Server running Docker container typing 127.0.0.1:28787 address bar, using rstudio username rstudio password. my_granolarr folder binded, everything save my_granolarr folder home directory RStudio Server saved computer. Everything else lost Docker container stopped.stop Docker container, running script Docker_RStudio_Stop.sh (Windows using PowerShell) repository folder.","code":"docker pull sdesabbata/granolarr:latest\ngit clone https://github.com/sdesabbata/granolarr.git\ncd granolarr\n./Docker_RStudio_Start.sh"},{"path":"preface.html","id":"session-info","chapter":"Preface","heading":"Session info","text":"Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nsessionInfo()## R version 4.1.1 (2021-08-10)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 20.04.3 LTS\n## \n## Matrix products: default\n## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.8.so\n## \n## locale:\n##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=C             \n##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] knitr_1.33      xml2_1.3.2      magrittr_2.0.1  downlit_0.2.1  \n##  [5] R6_2.5.1        rlang_0.4.11    fastmap_1.1.0   fansi_0.5.0    \n##  [9] stringr_1.4.0   tools_4.1.1     xfun_0.25       utf8_1.2.2     \n## [13] jquerylib_0.1.4 htmltools_0.5.2 ellipsis_0.3.2  yaml_2.2.1     \n## [17] digest_0.6.27   tibble_3.1.4    lifecycle_1.0.0 crayon_1.4.1   \n## [21] bookdown_0.23   sass_0.4.0      vctrs_0.3.8     fs_1.5.0       \n## [25] evaluate_0.14   rmarkdown_2.10  stringi_1.7.4   compiler_4.1.1 \n## [29] bslib_0.2.5.1   pillar_1.6.2    jsonlite_1.7.2  pkgconfig_2.0.3"},{"path":"introduction-to-r.html","id":"introduction-to-r","chapter":"1 Introduction to R","heading":"1 Introduction to R","text":"Print chapterIn first chapter, going provide brief introduction R, programming language focus module, well tool going use data science.","code":""},{"path":"introduction-to-r.html","id":"the-r-programming-language","chapter":"1 Introduction to R","heading":"1.1 The R programming language","text":"R created 1992 Ross Ihaka Robert Gentleman University Auckland, New Zealand. R free, open-source implementation S statistical programming language initially created Bell Labs. core, R functional programming language (main functionalities revolve around defining executing functions). However now supports, commonly used imperative (focused instructions variables programming control structures) object-oriented (involving complex object structures) programming language.simple terms, nowadays, programming R mostly focuses devising series instructions execute task – commonly, loading analysing dataset., R can used program creating sequences instructions involving variables – named entities can store values. main topic practical session. Instructions can include control flow structures, decision points (/else) loops, topic next practical session. Instructions can also grouped functions, also see next practical session.R interpreted, compiled. means R interpreter (using R Studio, R interpreter simply hidden backend R Studio frontend allows interact interpreter) receives instruction write R, interprets executes . programming languages require code compiled executable executed computer.","code":""},{"path":"introduction-to-r.html","id":"using-rstudio","chapter":"1 Introduction to R","heading":"1.1.1 Using RStudio","text":"open RStudio RStudio Server, interface divided two main sections. left side, find Console – well R script editor, script edited. Console input/output window R interpreter, instructions can typed, computed output shown.instance, type Consolethe R interpreter understands instruction sum one one, produces result (materials module created RMarkdown, output computation always preceded ‘##’).Note output value 2 preceded [1], indicates output constituted one element. output constituted one element, list numbers , row output preceded index first element output.right side, find two groups panels. top-right, main element Environment panel, representation current state interpreter’s memory, , shows stored variables, datasets, functions. bottom-right, find Files panel, shows file system (file folders computer server), well Help panel, shows help pages required. discuss panels later practical sessions.","code":"\n1 + 1## [1] 2##  [1]   1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324 361\n## [20] 400"},{"path":"introduction-to-r.html","id":"interpreting-values","chapter":"1 Introduction to R","heading":"1.2 Interpreting values","text":"value typed Console, interpreter simply returns value. examples , 2 simple numeric value, \"String value\" textual value, R referred character value programming also commonly referred string (short string characters).Numeric example:Character example:Note character values need start end single double quote (' \"), part information . Tidyverse Style Guide suggests always use double quote (\"), use module.Anything follows # symbol considered comment interpreter ignores .mentioned , interpreter understands simple operations numeric values.also large number pre-defined functions, e.g., square-root: sqrt.Functions collected stored libraries (sometimes referred packages), contains related functions. Libraries can range anywhere base library, includes sqrt function , rgdal library, contains implementations GDAL (Geospatial Data Abstraction Library) functionalities R.","code":"\n2## [1] 2\n\"String value\"## [1] \"String value\"\n# hi, I am a comment, please ignore me\n1 + 1## [1] 2\nsqrt(2)## [1] 1.414214"},{"path":"introduction-to-r.html","id":"variables","chapter":"1 Introduction to R","heading":"1.3 Variables","text":"variable can defined using identifier (e.g., a_variable) left assignment operator <-, followed object linked identifier, value (e.g., 1) assigned right. value variable can tested/invoked simply specifying identifier.type a_variable <- 1 Console RStudio, new element appears Environment panel, representing new variable memory. left part entry contains identifier a_variable, right part contains value assigned variable a_variable, 1.necessary provide value directly. right part assignment can call function. case, function executed provided input result assigned variable.Note type a_variable <- sqrt(4) Console RStudio, element Environment panel changes reflect new value assigned variable a_variable, now result sqrt(4), 2.example , another variable named another_variable created summed a_variable, saving result sum_of_two_variables. square root sum stored variable square_root_of_sum.","code":"\na_variable <- 1\na_variable## [1] 1\na_variable <- sqrt(4)\na_variable## [1] 2\nanother_variable <- 4\nanother_variable## [1] 4\nsum_of_two_variables <- a_variable + another_variable\n\nsquare_root_of_sum <- sqrt(sum_of_two_variables)\nsquare_root_of_sum## [1] 2.44949"},{"path":"introduction-to-r.html","id":"basic-types","chapter":"1 Introduction to R","heading":"1.4 Basic types","text":"","code":""},{"path":"introduction-to-r.html","id":"numeric","chapter":"1 Introduction to R","heading":"1.4.1 Numeric","text":"numeric type represents numbers (integers reals).Base numeric operators.pre-defined functions R:Use simple brackets specify order execution. specified default order : rise power first, multiplication division, sum subtraction last.object NaN (Number) returned R result operation number.confused object NA (Available), returned missing data.","code":"\na_number <- 1.41\nis.numeric(a_number)## [1] TRUE\nis.integer(a_number)## [1] FALSE\nis.double(a_number) # i.e., is real## [1] TRUE\nabs(-2) # Absolute value## [1] 2\nceiling(3.475) # Upper round## [1] 4\nfloor(3.475) # Lower round## [1] 3\ntrunc(5.99) # Truncate## [1] 5\nlog10(100) # Logarithm 10## [1] 2\nlog(exp(2)) # Natural logarithm and e## [1] 2\na_number <- 1\n(a_number + 2) * 3## [1] 9\na_number + (2 * 3)## [1] 7\na_number + 2 * 3## [1] 7\n0 / 0## [1] NaN\nis.nan(0 / 0)## [1] TRUE"},{"path":"introduction-to-r.html","id":"logical","chapter":"1 Introduction to R","heading":"1.4.2 Logical","text":"logical type encodes two truth values: True False.Basic logic operators","code":"\nlogical_var <- TRUE\nis.logical(logical_var)## [1] TRUE\nisTRUE(logical_var)## [1] TRUE\nas.logical(0) # TRUE if not zero## [1] FALSE"},{"path":"introduction-to-r.html","id":"character","chapter":"1 Introduction to R","heading":"1.4.3 Character","text":"character type represents text objects, including single characters character strings (text objects longer one character, commonly referred simply strings computer science).","code":"\na_string <- \"Hello world!\"\nis.character(a_string)## [1] TRUE\nis.numeric(a_string)## [1] FALSE\nas.character(2) # type conversion  (a.k.a. casting)## [1] \"2\"\nas.numeric(\"2\")## [1] 2\nas.numeric(\"Ciao\")## Warning: NAs introduced by coercion## [1] NA"},{"path":"introduction-to-r.html","id":"tidyverse","chapter":"1 Introduction to R","heading":"1.5 Tidyverse","text":"mentioned lecture, libraries collections functions /datasets. Libraries can installed R using function install.packages using Tool > Install Packages... RStudio.meta-library Tidyverse contains following libraries:ggplot2 system declaratively creating graphics, based Grammar Graphics. provide data, tell ggplot2 map variables aesthetics, graphical primitives use, takes care details.dplyr provides grammar data manipulation, providing consistent set verbs solve common data manipulation challenges.tidyr provides set functions help get tidy data. Tidy data data consistent form: brief, every variable goes column, every column variable.readr provides fast friendly way read rectangular data (like csv, tsv, fwf). designed flexibly parse many types data found wild, still cleanly failing data unexpectedly changes.purrr enhances R’s functional programming (FP) toolkit providing complete consistent set tools working functions vectors. master basic concepts, purrr allows replace many loops code easier write expressive.tibble modern re-imagining data frame, keeping time proven effective, throwing . Tibbles data.frames lazy surly: less complain forcing confront problems earlier, typically leading cleaner, expressive code.stringr provides cohesive set functions designed make working strings easy possible. built top stringi, uses ICU C library provide fast, correct implementations common string manipulations.forcats provides suite useful tools solve common problems factors. R uses factors handle categorical variables, variables fixed known set possible values.library can loaded using function library, shown (note name library quoted). library installed computer, don’t need install , every script needs load library uses. library loaded, functions can used.Important: always necessary load tidyverse meta-library want use stringr functions pipe operator %>%.","code":"\nlibrary(tidyverse)"},{"path":"introduction-to-r.html","id":"stringr","chapter":"1 Introduction to R","heading":"1.5.1 stringr","text":"code presents examples used lecture session demonstrate use stringr functions.","code":"\nstr_length(\"Leicester\")## [1] 9\nstr_detect(\"Leicester\", \"e\")## [1] TRUE\nstr_replace_all(\"Leicester\", \"e\", \"x\")## [1] \"Lxicxstxr\""},{"path":"introduction-to-r.html","id":"the-pipe-operator","chapter":"1 Introduction to R","heading":"1.5.2 The pipe operator","text":"pipe operator useful outline complex operations, step step (see also R Data Science, Chapter 18). pipe operator %>%takes result one functionand passes next functionas first argumentthat doesn’t need included code anymoreThe code shows simple example. number 2 taken input first pipe passes first argument function sqrt. output value 1.41 taken input second pipe, passes first argument function trunc. final output 1 finally returned.image graphically illustrates pipe operator works, compared procedure executed using two temporary variables used store temporary values.Illustration pipe operator worksThe first step sequence pipes can value, variable, function including arguments. code shows series examples different ways achieving result. examples use function round, also allows second argument digits = 2. Note , using pipe operator, nominally second argument provided function round – round(digits = 2)complex operation created use %>% can used right side <-, assign outcome operation variable.","code":"\n2 %>%\n  sqrt() %>%\n  trunc()## [1] 1\nsqrt(2) %>%\n round(digits = 2)\n# No pipe, using variables\ntmp_variable_A <- 2\ntmp_variable_B <- sqrt(tmp_variable_A)\nround(tmp_variable_B, digits = 2)\n\n# No pipe, using functions only\nround(sqrt(2), digits = 2)\n\n# Pipe starting from a value\n2 %>%\n  sqrt() %>%\n  round(digits = 2)\n\n# Pipe starting from a variable\nthe_value_two <- 2\nthe_value_two %>%\n  sqrt() %>%\n  round(digits = 2)\n\n# Pipe starting from a function\nsqrt(2) %>%\n round(digits = 2)\nsqrt_of_two <- 2 %>%\n  sqrt() %>%\n  round(digits = 2)"},{"path":"introduction-to-r.html","id":"coding-style","chapter":"1 Introduction to R","heading":"1.6 Coding style","text":"Study Tidyverse Style Guide (style.tidyverse.org) use consistently!","code":""},{"path":"introduction-to-r.html","id":"exercise-104.1","chapter":"1 Introduction to R","heading":"1.7 Exercise 104.1","text":"Question 104.1.1: Write piece code using pipe operator takes input number 1632, calculates logarithm base 10, takes highest integer number lower calculated value (lower round), verifies whether integer.Question 104.1.2: Write piece code using pipe operator takes input number 1632, calculates square root, takes lowest integer number higher calculated value (higher round), verifies whether integer.Question 104.1.3: Write piece code using pipe operator takes input string \"1632\", transforms number, checks whether result Number.Question 104.1.4: Write piece code using pipe operator takes input string \"-16.32\", transforms number, takes absolute value truncates , finally checks whether result Available.","code":""},{"path":"introduction-to-r.html","id":"exercise-104.2","chapter":"1 Introduction to R","heading":"1.8 Exercise 104.2","text":"Answer question , consulting stringr library reference (stringr.tidyverse.org/reference) necessaryQuestion 104.2.1: Write piece code using pipe operator stringr library takes input string \"like programming R\", transforms uppercase.Question 104.2.2: Write piece code using pipe operator stringr library takes input string \"like programming R\", truncates , leaving 10 characters.Question 104.2.3: Write piece code using pipe operator stringr library takes input string \"like programming R\", truncates , leaving 10 characters using ellipsis.Question 104.2.4: Write piece code using pipe operator stringr library takes input string \"like programming R\", manipulates leave string \"like R\".Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"r-programming.html","id":"r-programming","chapter":"2 R programming","heading":"2 R programming","text":"","code":""},{"path":"r-programming.html","id":"r-scripts","chapter":"2 R programming","heading":"2.1 R Scripts","text":"RStudio Console handy interact R interpreter obtain results operations commands. However, moving simple instructions actual program scripts conduct data analysis, Console usually sufficient anymore. fact, Console comfortable way providing long complex instructions interpreter editing past instructions want change something. better option create programs data analysis script significant size use RStudio integrated editor create R script.create R script, select top menu File > New File > R Script. opens embedded RStudio editor new empty R script folder. Copy two lines file. first loads tidyverse library, whereas second simply calculates square root two.top menu, select File > Save, type My_first_script.R (make sure include underscore .R extension) File name, click Save. first R script, congratulations!New lines code can added file, whole script can executed. Edit file adding line code shown , save . click Source button top-right editor execute file. happens first time? happens click Source ?Alternatively, can click specific line select one lines, click Run execute selected line(s).Delete two lines calculating square root two defining variable a_variable script, leaving line loading Tidyverse library. following sections, add code script execute , rather using Console.","code":"\n# Load the Tidyverse\nlibrary(tidyverse)\n\n# Calculate the square root of two\n2 %>% sqrt()## [1] 1.414214\n# First variable in a script\na_variable <- \"This is my first script\""},{"path":"r-programming.html","id":"vectors","chapter":"2 R programming","heading":"2.2 Vectors","text":"Vectors can defined R using function c, takes parameters items stored vector – stored order provided.vector created assigned identifier, elements within vector can retrieved specifying identifier, followed square brackets, index (indices see ) elements retrieved – remember indices start 1.retrieve subset vector (.e., just one element), specify integer vector containing indices interest (rather single integer value) square brackets.operator : can used create integer vectors, starting number specified operator number specified operator.functions seq rep can also used create vectors, illustrated .logical operators can used test conditional statements vector. former returns TRUE least one element satisfies statement, second returns TRUE elements satisfy conditionAll built-numerical functions R can used vector variable directly. , vector specified input, selected function applied element vector.","code":"\neast_midlands_cities <- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\nlength(east_midlands_cities)## [1] 4\n# Retrieve the third city\neast_midlands_cities[3]## [1] \"Lincoln\"\n# Retrieve first and third city\neast_midlands_cities[c(1, 3)]## [1] \"Derby\"   \"Lincoln\"\n# Create a vector containing integers between 2 and 4\ntwo_to_four <- 2:4\ntwo_to_four## [1] 2 3 4\n# Retrieve cities between the second and the fourth\neast_midlands_cities[two_to_four]## [1] \"Leicester\"  \"Lincoln\"    \"Nottingham\"\n# As the second element of two_to_four is 3...\ntwo_to_four[2]## [1] 3\n# the following command will retrieve the third city\neast_midlands_cities[two_to_four[2]]## [1] \"Lincoln\"\n# Create a vector with cities from the previous vector\nselected_cities <- c(east_midlands_cities[1], east_midlands_cities[3:4])\nseq(1, 10, by = 0.5)##  [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n## [16]  8.5  9.0  9.5 10.0\nseq(1, 10, length.out = 6)## [1]  1.0  2.8  4.6  6.4  8.2 10.0\nrep(\"Ciao\", 4)## [1] \"Ciao\" \"Ciao\" \"Ciao\" \"Ciao\"\nany(east_midlands_cities == \"Leicester\")## [1] TRUE\nmy_sequence <- seq(1, 10, length.out = 7)\nmy_sequence## [1]  1.0  2.5  4.0  5.5  7.0  8.5 10.0\nany(my_sequence > 5)## [1] TRUE\nall(my_sequence > 5)## [1] FALSE\none_to_ten <- 1:10\none_to_ten##  [1]  1  2  3  4  5  6  7  8  9 10\none_to_ten + 1##  [1]  2  3  4  5  6  7  8  9 10 11\nsqrt(one_to_ten)##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n##  [9] 3.000000 3.162278"},{"path":"r-programming.html","id":"filtering","chapter":"2 R programming","heading":"2.3 Filtering","text":"seen first practical session, conditional statement entered Console evaluated provided input, logical value (TRUE FALSE) provided output. Similarly, provided input vector, conditional statement evaluated element vector, vector logical values returned – contains respective results conditional statements element.subset elements vector can also selected providing vector logical values brackets identifier. new vector returned, containing values TRUE value specified correspondingly.result evaluating conditional statement vector vector logical values, can used filter vectors based conditional statements. conditional statement provided square brackets (vector identifier, instead index), new vector returned, contains elements conditional statement true.","code":"\nminus_three <- -3\nminus_three > 0## [1] FALSE\nminus_three_to_three <- -3:3\nminus_three_to_three## [1] -3 -2 -1  0  1  2  3\nminus_three_to_three > 0## [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\nminus_two_to_two <- -2:2\nminus_two_to_two## [1] -2 -1  0  1  2\nminus_two_to_two[c(TRUE, TRUE, FALSE, FALSE, TRUE)]## [1] -2 -1  2\nminus_two_to_two > 0## [1] FALSE FALSE FALSE  TRUE  TRUE\nminus_two_to_two[minus_two_to_two > 0]## [1] 1 2"},{"path":"r-programming.html","id":"conditional-statements","chapter":"2 R programming","heading":"2.4 Conditional statements","text":"Conditional statements fundamental (procedural) programming, allow execute execute part procedure depending whether certain condition true. condition tested part procedure execute case condition true included code block.simple conditional statement can created using example . complex structure can created using else, provide procedure execute case condition true, also alternative procedure, executed condition false.Finally, conditional statements can nested. , conditional statement can included part code block executed condition tested. instance, example , second conditional statement included code block executed case condition false.Similarly, first example seen lecture coded follows.","code":"\ntemperature <- 25\n\nif (temperature > 25) {\n  cat(\"It really warm today!\")\n}\ntemperature <- 12\n\nif (temperature > 25) {\n  cat(\"It really warm today!\")\n} else {\n  cat(\"Today is not warm\")\n}## Today is not warm\ntemperature <- -5\n\nif (temperature > 25) {\n  cat(\"It really warm today!\")\n} else {\n  if (temperature > 0) {\n    cat(\"There is a nice temperature today\")\n  } else {\n    cat(\"This is really cold!\")\n  }\n}## This is really cold!\na_value <- -7\n\nif (a_value == 0) {\n  cat(\"Zero\")\n} else {\n  if (a_value < 0) {\n    cat(\"Negative\") \n  } else {\n    cat(\"Positive\")\n  }\n}## Negative"},{"path":"r-programming.html","id":"loops","chapter":"2 R programming","heading":"2.5 Loops","text":"Loops another core component (procedural) programming implement idea solving problem executing task performing set steps number times. two main kinds loops R - deterministic conditional loops. former executed fixed number times, specified beginning loop. latter executed specific condition met. deterministic conditional loops extremely important working vectors.","code":""},{"path":"r-programming.html","id":"conditional-loops","chapter":"2 R programming","heading":"2.5.1 Conditional Loops","text":"R, conditional loops can implemented using repeat. difference two mostly syntactical: first tests condition first execute related code block condition true; second executes code block break command given (usually conditional statement).","code":"\na_value <- 0\n# Keep printing as long as x is smaller than 2\nwhile (a_value < 2) {\n  cat(a_value, \"\\n\")\n  a_value <- a_value + 1\n}## 0 \n## 1\na_value <- 0\n# Keep printing, if x is greater or equal than 2 than stop\nrepeat {\n  cat(a_value, \"\\n\")\n  a_value <- a_value + 1\n  if (a_value >= 2) break\n}## 0 \n## 1"},{"path":"r-programming.html","id":"deterministic-loops","chapter":"2 R programming","heading":"2.5.2 Deterministic Loops","text":"deterministic loop executes subsequent code block iterating elements provided vector. iteration (.e., execution code block), current element vector ( definition ) assigned variable statement ( definition ), can used code block., instance, possible iterate vector print elements.common practice create vector integers spot (e.g., using : operator) execute certain sequence steps pre-defined number times.","code":"for (<VAR> in <VECTOR>) {\n    ... code in loop ... \n    }\neast_midlands_cities <- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\nfor (city in east_midlands_cities){\n  cat(city, \"\\n\")\n}## Derby \n## Leicester \n## Lincoln \n## Nottingham\nfor (iterator in 1:3) {\n  cat(\"Exectuion number\", iterator, \":\\n\")\n  cat(\"    Step1: Hi!\\n\")\n  cat(\"    Step2: How is it going?\\n\")\n}## Exectuion number 1 :\n##     Step1: Hi!\n##     Step2: How is it going?\n## Exectuion number 2 :\n##     Step1: Hi!\n##     Step2: How is it going?\n## Exectuion number 3 :\n##     Step1: Hi!\n##     Step2: How is it going?"},{"path":"r-programming.html","id":"exercise-114.1","chapter":"2 R programming","heading":"2.6 Exercise 114.1","text":"Question 114.1.1: Use modulo operator %% create conditional statement prints \"Even\" number even \"Odd\" number odd.Question 114.1.2: Encapsulate conditional statement written Question 114.1.1 loop executes conditional statement numbers 1 10.Question 114.1.3: Encapsulate conditional statement written Question 114.1.1 loop prints name cities odd positions (.e., first, third, fifth) vector c(\"Birmingham\", \"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\", \"Wolverhampton\").Question 114.1.4: Write code necessary print name cities vector c(\"Birmingham\", \"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\", \"Wolverhampton\") many times position vector (.e., first city, two times second, forth).","code":""},{"path":"r-programming.html","id":"function-definition","chapter":"2 R programming","heading":"2.7 Function definition","text":"Recall lecture algorithm effective procedure mechanical rule, automatic method, programme performing mathematical operation (Cutland, 1980). program specific set instructions implement abstract algorithm. definition algorithm (thus program) can consist one functions, sets instructions perform task, possibly using input, possibly returning output value.code simple function one parameter. function simply calculates square root number. Add code script run portion script (type code Console).definition function executed, function becomes part environment, visible Environment panel, subsection titled Functions. Thereafter, function can called Console, portions script, well scripts.type instruction Console, add script run , function called using 27 argument, thus returning 3.","code":"\ncube_root <- function (input_value) {\n   result <- input_value ^ (1 / 3)\n   result\n}\ncube_root(27)## [1] 3"},{"path":"r-programming.html","id":"functions-and-control-structures","chapter":"2 R programming","heading":"2.7.1 Functions and control structures","text":"One issue writing functions making sure data given data right kind. example, happens try compute cube root negative number?probably wasn’t answer wanted. might remember NaN (Number) value return mathematical expression numerically indeterminate. case, actually due shortcoming ^ operator R, works positive base values. fact -7 perfectly valid cube root -343, since (-7)x(-7)x(-7) = -343.work around limitation, can state conditional rule:x < 0: calculate cube root x ‘normally’.Otherwise: work cube root positive number, change negative.kinds situations can dealt R function using statement, shown . Note operator - (.e., symbol minus) used obtain inverse number, way -1 inverse number 1.However, things can go wrong. example, cube_root(\"Leicester\") cause error occur, Error x^(1 / 3) : non-numeric argument binary operator. shouldn’t surprising cube roots make sense numbers, character variables. Thus, might helpful cube root function spot print warning explaining problem, rather just crashing fairly cryptic error message one , moment.function re-written making use .numeric second conditional statement. input value numeric, function returns value NA (Available) instead number. Note statement inside another statement, always possible nest code blocks – within within within within … etc.Finally, cat printing function, instructs R display provided argument (case, phrase within quotes) output Console. \\n cat tells R add newline printing warning.","code":"\ncube_root(-343)## [1] NaN\ncube_root <- function (input_value) {\n    if (input_value >= 0){\n        result <- input_value^(1 / 3) \n    }else{\n        result <- -( (-input_value)^(1/3) )\n    }\n    result\n}\n\ncube_root(343)\ncube_root(-343)\ncube_root <- function (input_value) { \n    if (is.numeric(input_value)) {\n        if (input_value >= 0){\n            result <- input_value^(1/3) \n        }else{\n            result <- -(-input_value)^(1/3)\n        }\n        result\n    }else{\n        cat(\"WARNING: Input variable must be numeric\\n\") \n        NA\n    }\n}"},{"path":"r-programming.html","id":"exercise-114.2","chapter":"2 R programming","heading":"2.8 Exercise 114.2","text":"Question 114.2.1: Write function calculates areas circle, taking radius first parameter.Question 114.2.2: Write function calculates volume cylinder, taking radius base first parameter height second parameter. function call function defined multiply returned value height calculate result.Question 114.2.3: Write function two parameters, vector numbers vector characters (text). function check input correct data type. numbers first vector greater zero, return elements second vector first length first vector.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"selection-and-manipulation.html","id":"selection-and-manipulation","chapter":"3 Selection and manipulation","heading":"3 Selection and manipulation","text":"","code":""},{"path":"selection-and-manipulation.html","id":"r-projects","chapter":"3 Selection and manipulation","heading":"3.1 R Projects","text":"RStudio provides extremely useful functionality organise code data, R Projects. specialised files RStudio can use store information specific project working – Environment, History, working directory, much , see coming weeks.RStudio Server, Files tab bottom-left panel, click Home make sure home folder – working computer, create folder practicals wherever convenient. Click New Folder enter Practicals prompt dialogue, create folder named Practicals.Select File > New Project … main menu, prompt menu, New Directory, New Project. Insert Practical_204 directory name, select Practicals folder field Create project subdirectory . Finally, click Create Project.RStudio now created project, activated . case, Files tab bottom-right panel Practical_204 folder, contains Practical_204.Rproj file. Practical_204.Rproj stores Environment information current project project files (e.g., R scripts, data, output files) stored within Practical_204 folder. Moreover, Practical_204 now working directory, means can refer file folder using name save file default directory save .top-right corner RStudio, see blue icon representing R cube, next name project (Practical_204). also indicates within Practical_204 project. Click Practical_204 select Close Project close project. Next R cube icon, now see Project: (None). Click Project: (None) select Practical_204 list reactivate Practical_204 project.Practical_204 project activated, select top menu File > New File > R Script. opens embedded RStudio editor new empty R script folder. Copy two lines file. first loads tidyverse library, whereas second loads another library code uses produce well-formatted tables.top menu, select File > Save, type My_script_Practical_204.R (make sure include underscore .R extension) File name, click Save.","code":"\nlibrary(tidyverse)\nlibrary(knitr)"},{"path":"selection-and-manipulation.html","id":"install-libraries","chapter":"3 Selection and manipulation","heading":"3.2 Install libraries","text":"RStudio RStudio Server come number libraries already pre-installed. However, might find position wanting install additional libraries work .remainder practical requires library nycflights13. install , select Tools > Install Packages… top menu. Insert nycflights13 Packages (separate multiple space comma) field click install. RStudio automatically execute command install.packages(\"nycflights13\") (, need execute ) install required library.usual, use function library load newly installed library.library nycflights13 contains dataset storing data flights departed New York City 2013. code , loads data frame flights library nycflights13 variable flights_from_nyc, using :: operator indicate data frame flights situated within library nycflights13.Add lines R script, well code snippets provided example .","code":"\nlibrary(nycflights13)\nflights_from_nyc <- nycflights13::flights"},{"path":"selection-and-manipulation.html","id":"loading-r-scripts","chapter":"3 Selection and manipulation","heading":"3.2.1 Loading R scripts","text":"furthermore possible load function(s) defined one script another script – fashion similar library loaded. Create new R script named Practical_204_RS_functions.R, copy code R script save fileCreate second R script named Practical_204_RS_main.R, copy code second R script save file.Executing Practical_204_RS_main.R instructs interpreter first run Practical_204_RS_functions.R script, thus creating cube_root function, invoke function using 27 argument, thus returning 3. simple example, can extremely powerful tool create library functions used different scripts.","code":"\ncube_root <- function (input_value) {\n   result <- input_value ^ (1 / 3)\n   result\n}\nsource(\"Practical_204_RS_functions.R\")\n\ncube_root(27)"},{"path":"selection-and-manipulation.html","id":"data-manipulation","chapter":"3 Selection and manipulation","heading":"3.3 Data manipulation","text":"analysis uses dplyr library (also part Tidyverse), offers grammar data manipulation.instance, function count can used count number rows data frame. code provides flights_from_nyc input function count pipe operator, thus creating new tibble one row one column.discussed previous lecture, tibble data type similar data frames, used Tidyverse libraries.Tidyverse functions output tibble rather data.frame objects representing table. However, data.frame object can provided input, automatically converted Tidyverse functions proceeding processing steps.tibble outputted count function , column n provides count. function kable library knitr used produce well-formatted table.example already shows pipe operator can used effectively multi-step operation.function count can also used count number rows table value given column, usually representing category.example , column name origin provided argument function count, rows representing flights origin counted together – EWR Newark Liberty International Airport, JFK John F. Kennedy International Airport, LGA LaGuardia Airport.can see, code formatted way similar code block, although code block. code goes new line every %>%, space added beginning new lines. common R programming (especially functions many parameters) makes code readable.","code":"\nflights_from_nyc %>%\n  dplyr::count() %>%\n  knitr::kable()\nflights_from_nyc %>%\n  dplyr::count(origin) %>%\n  knitr::kable()"},{"path":"selection-and-manipulation.html","id":"summarise","chapter":"3 Selection and manipulation","heading":"3.3.1 Summarise","text":"carry complex aggregations, function summarise can used combination function group_by summarise values rows data frame. Rows value selected column (example , origin) grouped together, values aggregated based defined function (using one columns calculation).example , function sum applied column distance calculate distance_traveled_from (total distance travelled flights starting airport).","code":"\nflights_from_nyc %>%\n  dplyr::group_by(origin) %>%\n  dplyr::summarise(\n    distance_traveled_from = sum(distance)\n  ) %>%\n  knitr::kable()"},{"path":"selection-and-manipulation.html","id":"select-and-filter","chapter":"3 Selection and manipulation","heading":"3.3.2 Select and filter","text":"function select can used select columns output. instance code , function select used select columns origin, dest, dep_delay, combination function slice_head, can used include first n rows (5 example ) output.function filter can instead used filter rows based specified condition. example , output filter step includes rows value month 11 (.e., eleventh month, November).Notice filter used combination select. functions dplyr library can combined, order makes logical sense. However, select step didn’t include month, column couldn’t used filter step.","code":"\nflights_from_nyc %>%\n  dplyr::select(origin, dest, dep_delay) %>%\n  dplyr::slice_head(n = 5) %>%\n  knitr::kable()\nflights_from_nyc %>%\n  dplyr::select(origin, dest, year, month, day, dep_delay) %>%\n  dplyr::filter(month == 11) %>%\n  dplyr::slice_head(n = 5) %>%\n  knitr::kable()"},{"path":"selection-and-manipulation.html","id":"mutate","chapter":"3 Selection and manipulation","heading":"3.3.3 Mutate","text":"function mutate can used add new column output table. mutate step code adds new column air_time_hours table obtained pipe, flight air time hours, dividing flight air time minutes 60.","code":"\nflights_from_nyc %>%\n  dplyr::select(flight, origin, dest, air_time) %>%\n  dplyr::mutate(\n    air_time_hours = air_time / 60\n  ) %>%\n  dplyr::slice_head(n = 5) %>%\n  knitr::kable()"},{"path":"selection-and-manipulation.html","id":"arrange","chapter":"3 Selection and manipulation","heading":"3.3.4 Arrange","text":"function arrange can used sort tibble ascending order values specified column. operator - specified column name, descending order used. code produce table showing rows ordered descending order air time.examples , used slice_head present first n (examples 5) rows table, based existing order. dplyr library also provides functions slice_max slice_min incorporate sorting functionality (see slice reference page)., following code uses slice_max produce table including 5 rows highest air time.following code, instead, uses slice_min, thus producing table including 5 rows lowest air time.cases, table contains ties, rows containing value present among maximum minimum selected values presented, case rows containing value 21 example .","code":"\nflights_from_nyc %>%\n  dplyr::select(flight, origin, dest, air_time) %>%\n  dplyr::arrange(-air_time) %>%\n  knitr::kable()\nflights_from_nyc %>%\n  dplyr::select(flight, origin, dest, air_time) %>%\n  dplyr::slice_max(air_time, n = 5) %>%\n  knitr::kable()\nflights_from_nyc %>%\n  dplyr::select(flight, origin, dest, air_time) %>%\n  dplyr::slice_min(air_time, n = 5) %>%\n  knitr::kable()"},{"path":"selection-and-manipulation.html","id":"data-manipulation-example","chapter":"3 Selection and manipulation","heading":"3.4 Data manipulation example","text":"Finally, code illustrates complex, multi-step operation using functions discussed .Start flights_from_nyc data.Select origin, destination, departure delay, year, month, day.Filter rows referring flights November.Filter rows departure delay (notice negation operator ! used) NA.necessary function mean return NA output values column NA.Group destination.Calculated average delay per destination.Add column delay calculated hours (minutes 60).Sort table descending delay (note - used column name).show first 5 rows.Create well-formatted table.","code":"\nflights_from_nyc %>%\n  dplyr::select(origin, dest, year, month, day, dep_delay) %>%\n  dplyr::filter(month == 11) %>%\n  dplyr::filter(!is.na(dep_delay)) %>%\n  dplyr::group_by(dest) %>%\n  dplyr::summarize(\n    avg_dep_delay = mean(dep_delay)\n  ) %>%\n  dplyr::mutate(\n    avg_dep_delay_hours = avg_dep_delay / 60\n  ) %>%\n  dplyr::arrange(-avg_dep_delay_hours) %>%\n  dplyr::slice_head(n = 5) %>%\n  knitr::kable()"},{"path":"selection-and-manipulation.html","id":"exercise-204.1","chapter":"3 Selection and manipulation","heading":"3.5 Exercise 204.1","text":"Extend code script My_script_Practical_204.R include code necessary solve questions .Question 204.1.1: Write piece code using pipe operator dplyr library generate table showing average air time hours, calculated grouping flights carrier, flights starting JFK airport.Question 204.1.2: Write piece code using pipe operator dplyr library generate table showing average arrival delay compared overall air time (tip: use manipulate create new column takes result arr_delay / air_time) calculated grouping flights carrier, flights starting JFK airport.Question 204.1.3: Write piece code using pipe operator dplyr library generate table showing average arrival delay compared overall air time calculated grouping flights origin destination, sorted destination.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"table-operations.html","id":"table-operations","chapter":"4 Table operations","heading":"4 Table operations","text":"section illustrates re-shape join functionalities Tidyverse libraries using simple examples. following sections instead present complex example, loading wrangling data related 2011 Output Area Classification Indexes Multiple Deprivation 2015.","code":"\nlibrary(tidyverse)\nlibrary(knitr)"},{"path":"table-operations.html","id":"table-manipulation","chapter":"4 Table operations","heading":"4.1 Table manipulation","text":"","code":""},{"path":"table-operations.html","id":"long-and-wide-formats","chapter":"4 Table operations","heading":"4.1.1 Long and wide formats","text":"Tabular data usually presented two different formats.Wide: common approach, real-world entity (e.g. city) represented one single row attributes represented different columns (e.g., column representing total population area, another column representing size area, etc.).Long: probably less common approach, still necessary many cases, real-world entity (e.g. city) represented multiple rows, one reporting one attributes. case, one column used indicate attribute row represent, another column used report value.tidyr library provides two functions allow transforming wide-formatted data long format, vice-versa. Please take time understand example check tidyr help pages continuing.","code":"\ncity_info_wide <- data.frame(\n    city = c(\"Leicester\", \"Nottingham\"),\n    population = c(329839, 321500),\n    area = c(73.3, 74.6),\n    density = c(4500, 4412)\n  ) %>%\n  tibble::as_tibble()\n\ncity_info_wide %>%\n knitr::kable()\ncity_info_long <- city_info_wide %>%\n  tidyr::pivot_longer(\n    # exclude IDs (city names) from the pivoted columns\n    cols = -city,\n    # name for the new column containing\n    # the names of the old columns\n    names_to = \"attribute\",\n    # name for the new column containing\n    # the values included under the old columns\n    values_to = \"value\"\n  )\n\ncity_info_long %>%\n  knitr::kable()\ncity_info_back_to_wide <- city_info_long %>%\n  tidyr::pivot_wider(\n    # column containing the attribute names\n    names_from = attribute,\n    # column containing the values\n    values_from = value\n  )\n\ncity_info_back_to_wide %>%\n  knitr::kable()"},{"path":"table-operations.html","id":"join","chapter":"4 Table operations","heading":"4.1.2 Join","text":"join operation combines two tables one matching rows values specified column. operation usually executed columns containing identifiers, matched different tables containing different data real-world entities. instance, table presents telephone prefixes two cities. information can combined data present wide-formatted table join operation columns containing city names. two tables contain cities, full join operation executed, cells values assigned.discussed lecture, dplyr library offers different types join operations, correspond different SQL joins illustrated image . use implications different types joins discussed detail GY7708 module next semester.Please take time understand example check related dplyr help pages continuing. first four examples execute exact full join operation using three different syntaxes: without using pipe operator, specifying argument . Note approaches writing join valid produce result. choice approach use depend code writing. particular, might find useful use syntax uses pipe operator join operation one stem series data manipulation steps. Using argument usually advisable unless certain aim join two tables exactly column names two table.Note result join operations saved variable. function knitr::kable added join operation pipe %>% display resulting table nice format.","code":"\ncity_telephone_prexix <- data.frame(\n    city = c(\"Leicester\", \"Birmingham\"),\n    telephon_prefix = c(\"0116\", \"0121\")\n  ) %>%\n  tibble::as_tibble()\n\ncity_telephone_prexix %>%\n  knitr::kable()\n# Option 1: without using the pipe operator\n\n# full join verb\ndplyr::full_join(\n    # left table\n    city_info_wide,\n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %>%\n  knitr::kable()\n# Option 2: without using the pipe operator\n#   and without using the argument \"by\"\n#   as columns have the same name\n#   in the two tables.\n# Same result as Option 1\n\n# full join verb\ndplyr::full_join(\n    # left table\n    city_info_wide,\n    # right table\n    city_telephone_prexix\n  ) %>%\n  knitr::kable()\n# Option 3: using the pipe operator\n#   and without using the argument \"by\"\n#   as columns have the same name\n#   in the two tables.\n# Same result as Option 1 and 2\n\n# left table\ncity_info_wide %>%\n  # full join verb\n  dplyr::full_join(\n    # right table\n    city_telephone_prexix\n  ) %>%\n  knitr::kable()\n# Option 4: using the pipe operator\n#   and using the argument \"by\".\n# Same result as Option 1, 2 and 3\n\n# left table\ncity_info_wide %>%\n  # full join verb\n  dplyr::full_join(\n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %>%\n  knitr::kable()\n# Left join\n# Using syntax similar to Option 1 above\n\n# left join\ndplyr::left_join(\n    # left table\n    city_info_wide, \n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %>%\n  kable()\n# Right join\n# Using syntax similar to Option 2 above\n\n# right join verb\ndplyr::right_join(\n    # left table\n    city_info_wide, \n    # right table\n    city_telephone_prexix\n  ) %>%\n  kable()\n# Inner join\n# Using syntax similar to Option 3 above\n\n# left table\ncity_info_wide %>%\n  # inner join\n  dplyr::inner_join(\n    # right table\n    city_telephone_prexix\n  ) %>%\n  kable()"},{"path":"table-operations.html","id":"read-and-write-data","chapter":"4 Table operations","heading":"4.2 Read and write data","text":"readr library (also part Tidyverse) provides series functions can used load save data different file formats.Download Blackboard (data folder repository) following files:2011_OAC_Raw_uVariables_Leicester.csvIndexesMultipleDeprivation2015_Leicester.csvCreate Practical_214 project make sure activated thus Practical_214 showing File tab bottom-right panel. Upload two files Practical_214 folder clicking Upload button selecting files computer (time writing, Chrome seems upload file correctly, whereas might necessary change names files upload using Microsoft Edge).Create new R script named Data_Wrangling_Example.R Practical_214 project, add library(tidyverse) first line. Use new script following sections practical session.2011 Output Area Classification (2011 OAC) geodemographic classification census Output Areas (OA) UK, created Gale et al. (2016) starting initial set 167 prospective variables United Kingdom Census 2011: 86 removed, 41 retained , 40 combined, leading final set 60 variables. Gale et al. (2016) finally used k-means clustering approach create 8 clusters supergroups (see map datashine.org.uk), well 26 groups 76 subgroups. dataset file 2011_OAC_Raw_uVariables_Leicester.csv contains original 167 variables, well resulting groups, city Leicester. full variable names can found file 2011_OAC_Raw_uVariables_Lookup.csv.Indexes Multiple Deprivation 2015 (see map cdrc.ac.uk) based series variables across seven distinct domains deprivation combined calculate Index Multiple Deprivation 2015 (IMD 2015). overall measure multiple deprivations experienced people living area. indexes calculated every Lower layer Super Output Area (LSOA), larger geographic unit OAs used 2011 OAC. dataset file IndexesMultipleDeprivation2015_Leicester.csv contains main Index Multiple Deprivation, well values seven distinct domains deprivation, two additional indexes regarding deprivation affecting children older people. dataset includes scores, ranks (1 indicates deprived area), decile (.e., first decile includes 10% deprived areas England).read_csv function reads Comma Separated Values (CSV) file path provided first argument. code loads 2011 OAC dataset. read_csv instruction throws warning shows assumptions data types used loading data. illustrated output last line code, data loaded tibble 969 x 190, 969 rows – one OA – 190 columns, 167 represent input variables used create 2011 OAC.code loads IMD 2015 dataset.function write_csv can used save dataset csv file. instance, code uses tidyverse functions pipe operator %>% :read 2011 OAC dataset directly file, without storing variable;select OA code variable OA11CD, two variables representing code name supergroup assigned OA 2011 OAC (supgrpcode supgrpname respectively);filter OA supergroup Suburbanites (code 6);write results file named Leicester_Suburbanites.csv.","code":"\nleicester_2011OAC <- \n  readr::read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n\nleicester_2011OAC %>% \n  dplyr::select(OA11CD,LSOA11CD, supgrpcode,supgrpname,Total_Population) %>%\n  dplyr::slice_head(n = 3) %>%\n  knitr::kable()\n# Load Indexes of Multiple deprivation data\nleicester_IMD2015 <- \n  readr::read_csv(\"IndexesMultipleDeprivation2015_Leicester.csv\")\nreadr::read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\") %>%\n  dplyr::select(OA11CD, supgrpcode, supgrpname) %>%\n  dplyr::filter(supgrpcode == 6) %>%\n  readr::write_csv(\"Leicester_Suburbanites.csv\")"},{"path":"table-operations.html","id":"file-paths","chapter":"4 Table operations","heading":"4.2.1 File paths","text":"File paths can specified two different ways:Absolute file path: full file path, root folder computer file.\nabsolute file path file can obtained using file.choose() instruction R Console, open interactive window allow select file computer. absolute path file printed console.\nAbsolute file paths provide direct link specific file ensure loading exact file.\nHowever, absolute file paths can problematic file moved, script run different system, file path invalid\nabsolute file path file can obtained using file.choose() instruction R Console, open interactive window allow select file computer. absolute path file printed console.Absolute file paths provide direct link specific file ensure loading exact file.However, absolute file paths can problematic file moved, script run different system, file path invalidRelative file path: partial path, current working folder file.\ncurrent working directory (current folder) part environment R session can identified using getwd() instruction `R Console*.\nnew R session started, current working directory usually computer user’s home folder.\nworking within R project, current working directory project directory.\ncurrent working can manually set specific directory using function setwd.\n\nUsing relative path working within R project option provides best overall consistency, assuming (data) files read scripts project also contained project folder (subfolder).\ncurrent working directory (current folder) part environment R session can identified using getwd() instruction `R Console*.\nnew R session started, current working directory usually computer user’s home folder.\nworking within R project, current working directory project directory.\ncurrent working can manually set specific directory using function setwd.\nnew R session started, current working directory usually computer user’s home folder.working within R project, current working directory project directory.current working can manually set specific directory using function setwd.Using relative path working within R project option provides best overall consistency, assuming (data) files read scripts project also contained project folder (subfolder).","code":"\n# Absolute file path\n# Note: the fist / indicates the root folder\nreadr::read_csv(\"/home/username/GY7702/data/2011_OAC_Raw_uVariables_Leicester.csv\")\n\n# Relative file path\n# assuming the working directory is the user home folder\n# /home/username\n# Note: no initial / for relative file paths\nreadr::read_csv(\"GY7702/data/2011_OAC_Raw_uVariables_Leicester.csv\")\n\n\n# Relative file path\n# assuming you are working within and R project created in the folder\n# /home/username/GY7702\n# Note: no initial / for relative file paths\nreadr::read_csv(\"data/2011_OAC_Raw_uVariables_Leicester.csv\")"},{"path":"table-operations.html","id":"data-wrangling-example","chapter":"4 Table operations","heading":"4.3 Data wrangling example","text":"","code":""},{"path":"table-operations.html","id":"re-shaping","chapter":"4 Table operations","heading":"4.3.1 Re-shaping","text":"IMD 2015 data long format, means every area represented one row: column Value presents value; column IndicesOfDeprivation indicates index value refers ; column Measurement indicates whether value score, rank, decile. code illustrates data format used IndicesOfDeprivation table, showing rows LSOA including University Leicester (feature code E01013649).following section, analysis aims explore certain census variables vary areas different deprivation levels. Thus, need extract Decile rows IMD 2015 dataset transform data wide format, index represented separate column.purpose, also need change name indexes slightly, exclude spaces punctuation, new column names simpler original text, can used column names. part manipulation performed using mutate functions stringr library.Let’s compare columns original long IMD 2015 dataset wide dataset created , using function colnames.leicester_IMD2015_decile_wide, now one row representing LSOA including University Leicester (feature code E01013649) main Index Multiple Deprivations now represented column IndexofMultipleDeprivationIMD. value reported – 5, means selected LSOA estimated range 40-50% deprived areas England – changed data format.","code":"\nleicester_IMD2015 %>%\n  dplyr::filter(FeatureCode == \"E01013649\") %>%\n  dplyr::select(FeatureCode, IndicesOfDeprivation, Measurement, Value) %>%\n  knitr::kable()\nleicester_IMD2015_decile_wide <- leicester_IMD2015 %>%\n  # Select only Socres\n  dplyr::filter(Measurement == \"Decile\") %>%\n  # Trim names of IndicesOfDeprivation\n  dplyr::mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\s\", \"\")\n  ) %>%\n  dplyr::mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"[:punct:]\", \"\")\n  ) %>%\n  dplyr::mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\(\", \"\")\n  ) %>%\n  dplyr::mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\)\", \"\")\n  ) %>%\n  # Spread\n  pivot_wider(\n    names_from = IndicesOfDeprivation,\n    values_from = Value\n  ) %>%\n  # Drop columns\n  dplyr::select(-DateCode, -Measurement, -Units)\nleicester_IMD2015 %>% \n  colnames() %>%\n  # limit width of printing area\n  print(width = 70)## [1] \"FeatureCode\"          \"DateCode\"            \n## [3] \"Measurement\"          \"Units\"               \n## [5] \"Value\"                \"IndicesOfDeprivation\"\nleicester_IMD2015_decile_wide %>% \n  colnames() %>%\n  # limit width of printing area\n  print(width = 70)##  [1] \"FeatureCode\"                                     \n##  [2] \"HealthDeprivationandDisabilityDomain\"            \n##  [3] \"IncomeDeprivationAffectingOlderPeopleIndexIDAOPI\"\n##  [4] \"BarrierstoHousingandServicesDomain\"              \n##  [5] \"EmploymentDeprivationDomain\"                     \n##  [6] \"EducationSkillsandTrainingDomain\"                \n##  [7] \"LivingEnvironmentDeprivationDomain\"              \n##  [8] \"IncomeDeprivationAffectingChildrenIndexIDACI\"    \n##  [9] \"CrimeDomain\"                                     \n## [10] \"IndexofMultipleDeprivationIMD\"                   \n## [11] \"IncomeDeprivationDomain\"\n# Original long IMD 2015 dataset\nleicester_IMD2015 %>%\n  dplyr::filter(\n    FeatureCode == \"E01013649\",\n    IndicesOfDeprivation == \"Index of Multiple Deprivation (IMD)\",\n    Measurement == \"Decile\"\n  ) %>%\n  dplyr::select(FeatureCode, IndicesOfDeprivation, Measurement, Value) %>%\n  knitr::kable()\n# New wide IMD 2015 dataset\nleicester_IMD2015_decile_wide %>%\n  dplyr::filter(FeatureCode == \"E01013649\") %>%\n  dplyr::select(FeatureCode, IndexofMultipleDeprivationIMD) %>%\n  knitr::kable()"},{"path":"table-operations.html","id":"join-1","chapter":"4 Table operations","heading":"4.3.2 Join","text":"discussed , two tables can joined using common column identifiers. can thus join 2011 OAC IMD 2015 datasets single table. LSOA code included 2011 OAC table used match information corresponding row IMD 2015. resulting table provides information 2011 OAC OA, plus Index Multiple Deprivations decile LSOA containing OA.operation can carried using function inner_join, specifying common column (columns, one used identifier) argument . Note using inner_join result dropping row doesn’t match table, either way. case, happen, OAs part LSOA, LSOA contains least one OA.LSOA contains multiple OAs, row leicester_IMD2015_decile_wide table matched multiple rows leicester_2011OAC table. instance, shown table , information IMD 2015 dataset LSOA encompassing University Leicester (feature code E01013649) joined multiple rows 2011 OAC dataset, including OA encompassing University Leicester (feature code E00068890) well neighbouring OAs.result stored variable leicester_2011OAC_IMD2015, analysis can carried . instance, count can used count many OAs fall 2011 OAC supergroup decile Index Multiple Deprivations.another example, code can used group OAs based decile calculate percentage adults employment using u074 (adults employment household: dependent children) u075 (adults employment household: dependent children) variables 2011 OAC dataset.","code":"\nleicester_2011OAC_IMD2015 <- \n  leicester_2011OAC %>%\n  inner_join(\n    leicester_IMD2015_decile_wide, \n    by = c(\"LSOA11CD\" = \"FeatureCode\")\n  )\nleicester_2011OAC_IMD2015 %>%\n  # Note that the LSOA11CD column needs to be used\n  # as the previous join as combined \n  # LSOA11CD and FeatureCode\n  # into one, name LSOA11CD\n  dplyr::filter(LSOA11CD == \"E01013649\") %>%\n  dplyr::select(OA11CD, LSOA11CD, supgrpname, IndexofMultipleDeprivationIMD) %>%\n  knitr::kable()\nleicester_2011OAC_IMD2015 %>%\n  dplyr::count(supgrpname, IndexofMultipleDeprivationIMD) %>%\n  knitr::kable()\nleicester_2011OAC_IMD2015 %>%\n  dplyr::group_by(IndexofMultipleDeprivationIMD) %>%\n  dplyr::summarise(\n    adults_not_empl_perc = (sum(u074 + u075) / sum(Total_Population)) * 100\n  ) %>%\n  knitr::kable()"},{"path":"table-operations.html","id":"exercise-214.1","chapter":"4 Table operations","heading":"4.4 Exercise 214.1","text":"Extend code script Data_Wrangling_Example.R include code necessary solve questions . Use full list variable names 2011 UK Census used generate 2011 OAC thatcan found file 2011_OAC_Raw_uVariables_Lookup.csv indetify columns use complete tasks.Question 214.1.1: Write piece code using pipe operator dplyr library generate table showing percentage EU citizens total population, calculated grouping OAs related decile Index Multiple Deprivations, accounting areas classified Cosmopolitans Ethnicity Central Multicultural Metropolitans.Question 214.1.2: Write piece code using pipe operator dplyr library generate table showing percentage EU citizens total population, calculated grouping OAs related supergroup 2011 OAC, accounting areas top 5 deciles Index Multiple Deprivations.Question 214.1.3: Write piece code using pipe operator dplyr library generate table showing percentage people aged 65 , calculated grouping OAs related supergroup 2011 OAC decile Index Multiple Deprivations, ordering table calculated value descending order.","code":""},{"path":"table-operations.html","id":"exercise-214.2","chapter":"4 Table operations","heading":"4.5 Exercise 214.2","text":"Extend code script Data_Wrangling_Example.R include code necessary solve questions .Question 214.2.1: Write piece code using pipe operator dplyr tidyr libraries generate long format leicester_2011OAC_IMD2015 table including values (census variables) used Question 214.1.3.Question 214.2.2: Write piece code using pipe operator dplyr tidyr libraries generate table similar one generated Question 214.2.1, showing values percentages total population.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"reproducibility-1.html","id":"reproducibility-1","chapter":"5 Reproducibility","heading":"5 Reproducibility","text":"","code":""},{"path":"reproducibility-1.html","id":"markdown","chapter":"5 Reproducibility","heading":"5.1 Markdown","text":"essential tool used creating materials RMarkdown R library allows create scripts mix Markdown mark-language R, create dynamic documents. RMarkdown script can compiled, point, Markdown notation interpreted create output files, R code executed output incorporated document.instance following markdown codeis rendered asThis link University Leicester bold.core Markdown notation used session presented . full RMarkdown cheatsheet available .","code":"[This is a link to the University of Leicester](http://le.ac.uk) and **this is in bold**.# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n\n**bold**\n*italics*\n\n[This is a link to the University of Leicester](http://le.ac.uk)\n\n- Example list\n    - Main folder\n        - Analysis\n        - Data\n        - Utils\n    - Other bullet point\n- And so on\n    - and so forth\n\n1. These are\n    1. Numeric bullet points\n    2. Number two\n2. Another number two\n3. This is number three"},{"path":"reproducibility-1.html","id":"r-markdown","chapter":"5 Reproducibility","heading":"5.1.1 R Markdown","text":"R code can embedded RMarkdown documents example . results code chunk displayed within document (echo=TRUE specified), followed output execution code.","code":"```{r, echo=TRUE}\na_number <- 0\na_number <- a_number + 1\na_number <- a_number + 1\na_number <- a_number + 1\na_number\n```\na_number <- 0\na_number <- a_number + 1\na_number <- a_number + 1\na_number <- a_number + 1\na_number## [1] 3"},{"path":"reproducibility-1.html","id":"exercise-224.1","chapter":"5 Reproducibility","heading":"5.2 Exercise 224.1","text":"Create new R project named Practical_224 directory name. Create RMarkdown document RStudio selecting File > New File > R Markdown … – might prompt RStudio update packages. RMarkdown document creation menu, specify “Practical 05” title name author, select PDF default output format.new document contain core document information, example , plus additional content simply explains RMarkdown works.Delete contents document information copy following text document information.option echo=TRUE tells RStudio include code output document, along output computation. echo=FALSE specified, code omitted. option message=FALSE warning=FALSE added, messages warnings R displayed output document.Save document selecting File > Save main menu. Enter Square_root file name click Save. file saved using Rmd (RMarkdown) extension.Click Knit button bar editor panel (top-left area) RStudio, left side. Check resulting pdf document. Try adding code (e.g., using examples ) Markdown text, compile document .","code":"---\ntitle: \"Practical 224\"\nauthor: \"A. Student\"\ndate: \"7 October 2018\"\noutput: pdf_document\n---# Pipe example\n\nThis is my first [RMarkdown](https://rmarkdown.rstudio.com/) document.\n\n```{r, echo=TRUE}\nlibrary(tidyverse)\n```\n\nThe code uses the pipe operator:\n\n- takes 2 as input\n- calculates the square root\n- rounds the value\n    - keeping only two digits\n\n```{r, echo=TRUE}\n2 %>%\n  sqrt() %>%\n  round(digits = 2)\n```"},{"path":"reproducibility-1.html","id":"exercise-224.2","chapter":"5 Reproducibility","heading":"5.3 Exercise 224.2","text":"Create analysis document based RMarkdown one two analyses seen practical sessions 3 4. two analyses, within respective R projects, first, create RMarkdown document. , add code related R script. Finally add additional content title, subtitles, importantly, text describing data used, analysis done, result obtained. Make sure add appropriate links data sources, available practical session materials.","code":""},{"path":"reproducibility-1.html","id":"git","chapter":"5 Reproducibility","heading":"5.4 Git","text":"Git free opensource version control system. commonly used server, master copy project kept, can also used locally. Git allows storing versions project, thus providing file synchronisation, consistency, history browsing, creation multiple branches. detailed introduction Git, please refer Pro Git book, written Scott Chacon Ben Straub.illustrated image , working git repository, common approach first check-latest version main repository start working file. series edits made, edits stage selected committed permanent snapshot. One commits can pushed main repository.Scott Chacon Ben Straub, licensed CC -NC-SA 3.0","code":""},{"path":"reproducibility-1.html","id":"git-and-rstudio","chapter":"5 Reproducibility","heading":"5.4.1 Git and RStudio","text":"RStudio Server, Files tab bottom-left panel, click Home make sure home folder – working computer, create folder granolarr wherever convenient. Click New Folder enter Repos (short repositories) prompt dialogue, create folder named Repos.Create GitHub account github.com, don’t one, create new repository named -granolarr, following instructions available GitHub help pages. Make sure tick box next Initialize repository README, adds README.md markdown file repository.repository created, GitHub take repository page. Copy link repository .git file clicking green Clone download button copying https URL available. Back RStudio Server, select File > New Project… top menu select Version Control Git New Project panel. Paste copied URL Repository URL field, select Repos folder created folder Create project subdirectory , click Create Project. RStudio might ask GitHub username password point.continuing, need record identity Git system installed RStudio Server, able communicate GitHub’s server. Open Terminal tab RStudio (visible, select Tools > Terminal > New Terminal top menu). First, paste command substituting @example.com university email (make sure maintain double quotes) press return button., paste command substituting Name name (make sure maintain double quotes) press return button.RStudio now switched new R project linked -granolarr repository. RStudio File tab bottom-right panel, navigate file created exercises , select files copy folder new project (File tab, > Copy …).Check now available Git tab top-right panel, see least newly copied files marked untracked. Tick checkboxes Staged column stage files, click Commit button.newly opened panel Commit window, top-left section shows files, bottom section shows edits. Write first commit Commit message section top-right, click Commit button. pop-notify completed commit. Close pop-panel, click Push button top-right Commit window. Another pop-panel ask GitHub username password show executed push operation. Close pop-panel Commit window.Congratulations, completed first commit! Check repository page GitHub. reload page, top bar show 2 commits left files now visible file list . click 2 commits, can see commit history, including initial commit created repository commit just completed.","code":"git config --global user.email \"you@example.com\"git config --global user.name \"Your Name\""},{"path":"reproducibility-1.html","id":"exercise-224.3","chapter":"5 Reproducibility","heading":"5.5 Exercise 224.3","text":"Create new GitHub repository named GY7702_224_Exercise3. Clone repository RStudio Server new R project repository. Create RMarkdown document exploring presence different living arrangements Leicester among different categories 2011 Output Area Classification deciles Index Multiple Deprivations, copying required data project folder.analysis include:introduction data aims project;justification analysis methods;code related results;discussion results within document.","code":""},{"path":"reproducibility-1.html","id":"cloning-granolarr","chapter":"5 Reproducibility","heading":"5.6 Cloning granolarr","text":"can follow steps listed clone granolarr repository.Create folder named Repos home directory. working RStudio Server, Files panel, click Home button (second bar, next house icon), click New Folder, enter name Repos click Ok.Create folder named Repos home directory. working RStudio Server, Files panel, click Home button (second bar, next house icon), click New Folder, enter name Repos click Ok.RStudio RStudio Server select File > New Project...RStudio RStudio Server select File > New Project...Select Version Control Git (might need set Git first working computer)Select Version Control Git (might need set Git first working computer)Copy https://github.com/sdesabbata/granolarr.git Repository URL field select Repos folder field Create project subdirectory , click Create Project.Copy https://github.com/sdesabbata/granolarr.git Repository URL field select Repos folder field Create project subdirectory , click Create Project.look aroundHave look aroundClick project name granolarr top-left interface select Close Project close project.Click project name granolarr top-left interface select Close Project close project.granolarr public repository, can clone , edit wish push copy repository. However, contributing edits original repository require steps. Check GitHub help pages interested.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"6 Exploratory data analysis","heading":"6 Exploratory data analysis","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"introduction","chapter":"6 Exploratory data analysis","heading":"6.1 Introduction","text":"practical showcases exploratory analysis distribution people aged 20 24 Leicester, using u011 variable 2011 Output Area Classification (2011OAC) dataset. Create new R project practical session create new RMarkdown document replicate analysis document.document set , start adding first R code snipped including code , loads 2011OAC dataset libraries used practical session.","code":"\nlibrary(tidyverse)\nlibrary(knitr)\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")"},{"path":"exploratory-data-analysis.html","id":"gglot2-recap","chapter":"6 Exploratory data analysis","heading":"6.2 GGlot2 recap","text":"seen practical session 401, ggplot2 library part Tidyverse, offers series functions creating graphics declaratively, based concepts outlined Grammar Graphics. dplyr library offers functionalities cover data manipulation variable transformations, ggplot2 library offers functionalities allow specify elements, define guides, apply scale coordinate system transformations.Marks can specified ggplot2 using geom_ functions.mapping variables (table columns) visual variables can specified ggplot2 using aes element.Furthermore, ggplot2 library:\nautomatically adds necessary guides using default table column names, additional functions can used overwrite defaults;\nprovides wide range scale_ functions can used control scales visual variables;\nprovides series coord_ fucntions allow transforming coordinate system.\nautomatically adds necessary guides using default table column names, additional functions can used overwrite defaults;provides wide range scale_ functions can used control scales visual variables;provides series coord_ fucntions allow transforming coordinate system.Check ggplot2 reference details functions options discussed .","code":""},{"path":"exploratory-data-analysis.html","id":"data-visualisation","chapter":"6 Exploratory data analysis","heading":"6.3 Data visualisation","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"distributions","chapter":"6 Exploratory data analysis","heading":"6.3.1 Distributions","text":"start analysis simple histogram, explore distribution variable u011. RMarkdown allows specifying height (well width) figure option R snipped, shown example typed plain text .snipped barchart included output documents, shown .aim explore portion population distributed among different supergroups 2011OAC, number charts allow us visualise relationship.instance, barchart can enhanced use visual variable colour fill option. graphic uses options seen practical session 401 create stacked barchart, sections bar filled colour associated 2011OAC supergroup.However, graphic extremely clear. boxplot violin plot created data shown . cases, parameter axis.text.x function theme set element_text(angle = 90, hjust = 1) order orientate labels x-axis vertically, supergroup names rather long, overlap one-another set horizontally x-axis. cases, option fig.height R snippet RMarkdown set higher value (e.g., 5) allow sufficient room supergroup names.","code":"```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.height = 4}\nleicester_2011OAC %>%\n  ggplot2::ggplot(\n    aes(\n      x = u011\n    )\n  ) +\n  ggplot2::geom_histogram(binwidth = 5) +\n  ggplot2::theme_bw()\n```\nleicester_2011OAC %>%\n  ggplot2::ggplot(\n    aes(\n      x = u011\n    )\n  ) +\n  ggplot2::geom_histogram(binwidth = 5) +\n  ggplot2::theme_bw()\nleicester_2011OAC %>%\n  ggplot2::ggplot(\n    aes(\n      x = u011,\n      fill = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  ggplot2::geom_histogram(binwidth = 5) +\n  ggplot2::ggtitle(\"Leicester's young adults\") +\n  ggplot2::labs(\n    fill = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  ggplot2::xlab(\"Residents aged 20 to 24\") +\n  ggplot2::ylab(\"Count\") +\n  ggplot2::scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  ggplot2::theme_bw()\nleicester_2011OAC %>%\n  ggplot2::ggplot(\n    aes(\n      x = fct_reorder(supgrpname, supgrpcode),\n      y = u011,\n      fill = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  ggplot2::geom_boxplot() +\n  ggtitle(\"Leicester's young adults\") +\n  ggplot2::labs(\n    fill = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  ggplot2::xlab(\"2011 Output Area Classification (supergroups)\") +\n  ggplot2::ylab(\"Residents aged 20 to 24\") +\n  ggplot2::scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(axis.text.x = element_text(angle = 90, hjust = 1))\nleicester_2011OAC %>%\n  ggplot2::ggplot(\n    aes(\n      x = fct_reorder(supgrpname, supgrpcode),\n      y = u011,\n      fill = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  ggplot2::geom_violin() +\n  ggtitle(\"Leicester's young adults\") +\n  ggplot2::labs(\n    fill = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  ggplot2::xlab(\"2011 Output Area Classification (supergroups)\") +\n  ggplot2::ylab(\"Residents aged 20 to 24\") +\n  ggplot2::scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(axis.text.x = element_text(angle = 90, hjust = 1))"},{"path":"exploratory-data-analysis.html","id":"relationships","chapter":"6 Exploratory data analysis","heading":"6.3.2 Relationships","text":"first barchart seems illustrate distribution might skewed towards left, values seemingly 50. However, tells part story people aged 20 24 distributed Leicester. fact, Output Area (OA) different total population. , higher number people aged 20 24 living OA might simply due OA populous others. Thus, next step compare u011 Total_Population, instance, scatterplot one seen practical session 401, reported .","code":"\nleicester_2011OAC %>%\n  ggplot2::ggplot(\n    aes(\n      x = Total_Population,\n      y = u011,\n      colour = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  ggplot2::geom_point(size = 0.5) +\n  ggplot2::ggtitle(\"Leicester's young adults\") +\n  ggplot2::labs(\n    colour = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  ggplot2::xlab(\"Total number of residents\") +\n  ggplot2::ylab(\"Residents aged 20 to 24\") +\n  ggplot2::scale_y_log10() +\n  ggplot2::scale_colour_brewer(palette = \"Set1\") +\n  ggplot2::scale_colour_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  ggplot2::theme_bw()"},{"path":"exploratory-data-analysis.html","id":"exercise-304.1","chapter":"6 Exploratory data analysis","heading":"6.4 Exercise 304.1","text":"Question 304.1.1: one boxplot violin plot think better illustrate different distributions, two graphics say distribution people aged 20 24 Leicester? Write short answer RMarkdown document (max 200 words).Question 304.1.2: Create jittered points plot (see geom_jitter) visualisation illustrating data shown boxplot violin plot .Question 304.1.3: Create code necessary calculate new column named perc_age_20_to_24, percentage people aged 20 24 (.e., u011) total population per OA Total_Population, create boxplot visualising distribution variable per 2011OAC supergroup.","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-statistics","chapter":"6 Exploratory data analysis","heading":"6.5 Exploratory statistics","text":"graphics provide preliminary evidence distribution people aged 20 24 might, fact, different different 2011 supergroups. remainder practical session, going explore hypothesis . First, load necessary statistical libraries.code calculates percentage people aged 20 24 (.e., u011) total population per OA, also recodes (see recode) names 2011OAC supergroups shorter 2-letter version, useful tables presented .OA code, recoded 2011OAC supergroup name, newly created perc_age_20_to_24 retained new table leic_2011OAC_20to24. step sometimes useful stepping stone analysis can make code easier read line. Sometimes also necessary step interacting certain libraries, fully compatible Tidyverse libraries, leveneTest.","code":"\nleic_2011OAC_20to24 <- leicester_2011OAC %>%\n  dplyr::mutate(\n    perc_age_20_to_24 = (u011 / Total_Population) * 100,\n    supgrpname = dplyr::recode(supgrpname, \n      `Suburbanites` = \"SU\",\n      `Cosmopolitans` = \"CP\",\n      `Multicultural Metropolitans` = \"MM\",\n      `Ethnicity Central` = \"EC\",\n      `Constrained City Dwellers` = \"CD\",\n      `Hard-Pressed Living` = \"HP\",\n      `Urbanites` = \"UR\"\n    )\n  ) %>%\n  dplyr::select(OA11CD, supgrpname, perc_age_20_to_24)\n\nleic_2011OAC_20to24 %>%\n  dplyr::slice_head(n = 5) %>%\n  knitr::kable()"},{"path":"exploratory-data-analysis.html","id":"descriptive-statistics","chapter":"6 Exploratory data analysis","heading":"6.5.1 Descriptive statistics","text":"first step statistical analysis modelling explore “shape” data involved, looking descriptive statistics variables involved. function stat.desc pastecs library provides three series descriptive statistics.base:\nnbr.val: overall number values dataset;\nnbr.null: number NULL values – NULL often returned expressions functions whose values undefined;\nnbr.na: number NAs – missing value indicator;\nnbr.val: overall number values dataset;nbr.null: number NULL values – NULL often returned expressions functions whose values undefined;nbr.na: number NAs – missing value indicator;desc:\nmin (see also min function): minimum value dataset;\nmax (see also max function): minimum value dataset;\nrange: difference min max (different range());\nsum (see also sum function): sum values dataset;\nmedian (see also median function): median, value separating higher half lower half values\nmean (see also mean function): arithmetic mean, sum number values NA;\nSE.mean: standard error mean – estimation variability mean calculated different samples data (see also central limit theorem);\nCI.mean.0.95: 95% confidence interval mean – indicates 95% probability actual mean within distance sample mean;\nvar: variance (\\(\\sigma^2\\)), quantifies amount variation average squared distances mean;\nstd.dev: standard deviation (\\(\\sigma\\)), quantifies amount variation square root variance;\ncoef.var: variation coefficient quantifies amount variation standard deviation divided mean;\nmin (see also min function): minimum value dataset;max (see also max function): minimum value dataset;range: difference min max (different range());sum (see also sum function): sum values dataset;median (see also median function): median, value separating higher half lower half valuesmean (see also mean function): arithmetic mean, sum number values NA;SE.mean: standard error mean – estimation variability mean calculated different samples data (see also central limit theorem);CI.mean.0.95: 95% confidence interval mean – indicates 95% probability actual mean within distance sample mean;var: variance (\\(\\sigma^2\\)), quantifies amount variation average squared distances mean;std.dev: standard deviation (\\(\\sigma\\)), quantifies amount variation square root variance;coef.var: variation coefficient quantifies amount variation standard deviation divided mean;norm (default FALSE, use norm = TRUE include output):\nskewness: skewness value indicates\npositive: distribution skewed towards left;\nnegative: distribution skewed towards right;\n\nkurtosis: kurtosis value indicates:\npositive: heavy-tailed distribution;\nnegative: flat distribution;\n\nskew.2SE kurt.2SE: skewness kurtosis divided 2 standard errors. greater 1, respective statistics significant (p < .05);\nnormtest.W: test statistics Shapiro–Wilk test normality;\nnormtest.p: significance Shapiro–Wilk test normality.\nskewness: skewness value indicates\npositive: distribution skewed towards left;\nnegative: distribution skewed towards right;\npositive: distribution skewed towards left;negative: distribution skewed towards right;kurtosis: kurtosis value indicates:\npositive: heavy-tailed distribution;\nnegative: flat distribution;\npositive: heavy-tailed distribution;negative: flat distribution;skew.2SE kurt.2SE: skewness kurtosis divided 2 standard errors. greater 1, respective statistics significant (p < .05);normtest.W: test statistics Shapiro–Wilk test normality;normtest.p: significance Shapiro–Wilk test normality.Shapiro–Wilk test compares distribution variable normal distribution mean standard deviation. null hypothesis Shapiro–Wilk test sample normally distributed, thus normtest.p lower 0.01 (.e., p < .01), test indicates distribution probably normal. threshold accept reject hypothesis arbitrary based conventions, p < .01 commonly accepted threshold, p < .05 relatively small data sample (e.g., 30 cases).next step thus apply stat.desc variable currently exploring (.e., perc_age_20_to_24), including norm section.table tells us 969 OA Leicester valid value variable perc_age_20_to_24, NULL NA value found. values vary 1% almost 61%, average value 11% population OA aged 20 24.short paragraph reporting values table, taking advantage two features RMarkdown. First, output stat.desc function snippet stored variable leic_2011OAC_20to24_stat_desc, valid variable rest document. Second, RMarkdown allows -line R snippets, can also refer variables defined snippet text. , source paragraph reads , -line R snipped opened single grave accent (.e., `) followed lowercase r closed another single grave accent.included code RMarkdown document, copy text verbatim RMarkdown document make sure understand code -line R snippets works.data described statistics presented table random sample population, 95% confidence interval CI.mean.0.95 indicate can 95% confident actual mean distribution somewhere 10.566 - 0.596 = 9.97% 10.566 + 0.596 = 11.162%.However, sample. Thus statistical interpretation valid, way sum values doesn’t make sense, sum series percentages.skew.2SE kurt.2SE greater 1, indicate skewness kurtosis values significant (p < .05). skewness positive, indicates distribution skewed towards left (low values). kurtosis positive, indicates distribution heavy-tailed., perc_age_20_to_24 heavy-tailed distribution skewed towards low values, surprising normtest.p value indicates Shapiro–Wilk test significant, indicates distribution normal.code present output shapiro.test function, present outcome Shapiro–Wilk test values provided input. output values values reported norm section stat.desc. Note shapiro.test function require argument numeric vector. Thus pull function must used extract perc_age_20_to_24 column leic_2011OAC_20to24 vector, whereas using select single column name argument produce output table single column.two code snippets can used visualise density-based histogram including shape normal distribution mean standard deviation, Q-Q plot, visually confirm fact perc_age_20_to_24 normally distributed.Q-Q plot R can created using variety functions. example , plot created using stat_qq stat_qq_line functions ggplot2 library. Note perc_age_20_to_24 variable mapped particular option aes sample.perc_age_20_to_24 normally distributed, dots Q-Q plot distributed straight line included plot.","code":"\nleic_2011OAC_20to24_stat_desc <- leic_2011OAC_20to24 %>%\n  dplyr::select(perc_age_20_to_24) %>%\n  pastecs::stat.desc(norm = TRUE)\n  \nleic_2011OAC_20to24_stat_desc %>%\n  knitr::kable(digits = 3)The table above tells us that all `r \"\\u0060r leic_2011OAC_20to24_stat_desc[\\\"nbr.val\\\",\n\\\"perc_age_20_to_24\\\"] %>% round(digits = 0)\\u0060\"` OA in Leicester have a valid \nvalue for the variable `perc_age_20_to_24`, as no `r \"\\u0060NULL\\u0060\"` nor `r \n\"\\u0060NA\\u0060\"` value have been found.The values vary from about `r \"\\u0060r \nleic_2011OAC_20to24_stat_desc[\\\"min\\\", \\\"perc_age_20_to_24\\\"] %>% round(digits = \n0)\\u0060\"`% to almost `r \"\\u0060r leic_2011OAC_20to24_stat_desc[\\\"max\\\", \n\\\"perc_age_20_to_24\\\"] %>% round(digits = 0)\\u0060\"`%, with an average value of \n`r \"\\u0060r leic_2011OAC_20to24_stat_desc[\\\"mean\\\", \\\"perc_age_20_to_24\\\"] %>% \nround(digits = 0)\\u0060\"`% of the population in an OA aged between 20 and 24. \nleic_2011OAC_20to24 %>%\n  dplyr::pull(perc_age_20_to_24) %>%\n  stats::shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.64491, p-value < 2.2e-16\nleic_2011OAC_20to24 %>%\n  ggplot2::ggplot(\n    aes(\n      x = perc_age_20_to_24\n    )\n  ) +\n  ggplot2::geom_histogram(\n    aes(\n      y =..density..\n    ),\n    binwidth = 5\n  ) + \n  ggplot2::stat_function(\n    fun = dnorm, \n    args = list(\n      mean = leic_2011OAC_20to24 %>% pull(perc_age_20_to_24) %>% mean(),\n      sd = leic_2011OAC_20to24 %>% pull(perc_age_20_to_24) %>% sd()\n    ),\n    colour = \"red\", size = 1\n  )\nleic_2011OAC_20to24 %>%\n  ggplot2::ggplot(\n    aes(\n      sample = perc_age_20_to_24\n    )\n  ) +\n  ggplot2::stat_qq() +\n  ggplot2::stat_qq_line()"},{"path":"exploratory-data-analysis.html","id":"exercise-304.2","chapter":"6 Exploratory data analysis","heading":"6.6 Exercise 304.2","text":"Create new RMarkdown document, add code necessary recreate table leic_2011OAC_20to24 used example . Use code re-shape table leic_2011OAC_20to24 pivoting perc_age_20_to_24 column wider multiple columns using supgrpname new column names.manipulation creates one column per supergroup, containing perc_age_20_to_24 OA part supergroup, NA value OA part supergroup. transformation illustrated two tables . first shows extract original leic_2011OAC_20to24 dataset, followed wide version leic_2011OAC_20to24_supgrp.Question 304.2.1: code uses newly created leic_2011OAC_20to24_supgrp table calculate descriptive statistics calculated variable leic_2011OAC_20to24 supergroup. leic_2011OAC_20to24 normally distributed subgroups? yes, supergroups based values justify claim? (Write 200 words)Question 304.2.2: Write code necessary test normality leic_2011OAC_20to24 supergroups analysis conducted Question 304.2.1 indicated normal, using function shapiro.test, draw respective Q-Q plot.Question 304.2.3: Observe output Levene’s test executed . result tell variance perc_age_20_to_24 supergroups?Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nleic_2011OAC_20to24_supgrp <- leic_2011OAC_20to24 %>%\n  tidyr::pivot_wider(\n    names_from = supgrpname,\n    values_from = perc_age_20_to_24\n  )\nleic_2011OAC_20to24 %>%\n  dplyr::slice_min(OA11CD, n = 10) %>%\n  knitr::kable(digits = 3)\nleic_2011OAC_20to24_supgrp %>%\n  dplyr::slice_min(OA11CD, n = 10) %>%\n  knitr::kable(digits = 3)\nleic_2011OAC_20to24_supgrp %>%\n  dplyr::select(-OA11CD) %>%\n  pastecs::stat.desc(norm = TRUE) %>%\n  knitr::kable(digits = 3)\ncar::leveneTest(leic_2011OAC_20to24$perc_age_20_to_24, leic_2011OAC_20to24$supgrpname)## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value    Pr(>F)    \n## group   6  62.011 < 2.2e-16 ***\n##       962                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"comparing-data.html","id":"comparing-data","chapter":"7 Comparing data","heading":"7 Comparing data","text":"","code":""},{"path":"comparing-data.html","id":"introduction-1","chapter":"7 Comparing data","heading":"7.1 Introduction","text":"first part practical guides ANOVA (analysis variance) regression analysis seen lecture, last part showcases multiple regression analysis. Create new R project practical session create new RMarkdown document replicate analysis document separate RMarkdown document work exercises.many functions used analyses part oldest libraries developed R, developed easily compatible Tidyverse %>% operator. Fortunately, magrittr library (loaded ) define %>% operator seen far, also exposition pipe operator %$%, exposes columns data.frame left operator expression right operator. , %$% allows refer column data.frame directly subsequent expression. , lines expose column Petal.Length data.frame iris pass mean function using different approaches, equivalent outcome.","code":"\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(knitr)\n# Classic R approach\nmean(iris$Petal.Length) ## [1] 3.758\n# Using %>% pipe\niris$Petal.Length %>% \n  mean()  ## [1] 3.758\n# Using %>% pipe and %$% exposition pipe\niris %$% Petal.Length %>% \n  mean() ## [1] 3.758"},{"path":"comparing-data.html","id":"anova","chapter":"7 Comparing data","heading":"7.2 ANOVA","text":"ANOVA (analysis variance) tests whether values variable (e.g., length petal) average different different groups (e.g., different species iris). ANOVA developed generalised version t-test, objective allows test two groups.ANOVA test following assumptions:normally distributed values groups\nespecially groups different sizes\nespecially groups different sizeshomogeneity variance values groups\ngroups different sizes\ngroups different sizesindependence groups","code":""},{"path":"comparing-data.html","id":"example","chapter":"7 Comparing data","heading":"7.2.1 Example","text":"example seen lecture illustrates ANOVA can used verify three different species iris iris dataset different petal length.ANOVA considered robust test, thus, groups size, need test homogeneity variance. Furthermore, groups come different species flowers, need test independence values. assumption needs testing whether values three groups normally distributed. 50 flowers per species, can set significance threshold 0.05.three Shapiro–Wilk tests significant, indicates three groups normally distributed values.can thus conduct ANOVA test using function aov, function summary obtain summary results test.difference significant F(2, 147) = 1180.16, p < .01.image highlights important values output: significance value Pr(>F); F-statistic value F value; two degrees freedom values F-statistic Df column.","code":"\niris %>%\n  ggplot2::ggplot(\n    aes(\n      x = Species, \n      y = Petal.Length\n    )\n  ) +\n  ggplot2::geom_boxplot()\niris %>% dplyr::filter(Species == \"setosa\") %>% dplyr::pull(Petal.Length) %>% stats::shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.95498, p-value = 0.05481\niris %>% dplyr::filter(Species == \"versicolor\") %>% dplyr::pull(Petal.Length) %>% stats::shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.966, p-value = 0.1585\niris %>% dplyr::filter(Species == \"virginica\") %>% dplyr::pull(Petal.Length) %>% stats::shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.96219, p-value = 0.1098\n# Classic R coding approach (not using %$%)\n# iris_anova <- aov(Petal.Length ~ Species, data = iris)\n# summary(iris_anova)\n\niris %$%\n  stats::aov(Petal.Length ~ Species) %>%\n  summary()##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Species       2  437.1  218.55    1180 <2e-16 ***\n## Residuals   147   27.2    0.19                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"comparing-data.html","id":"exercise-314.1","chapter":"7 Comparing data","heading":"7.3 Exercise 314.1","text":"Question 314.1.1: Load 2011_OAC_Raw_uVariables_Leicester.csv dataset. Check whether values mean age (u020) normally distributed, whether can transformed normally distributed set using logarithmic inverse hyperbolic sine functions.Question 314.1.2: Check whether values mean age (u020) normally distributed looking different 2011OAC supergroups separately. Check whether can transformed normally distributed set using logarithmic inverse hyperbolic sine functions.Question 314.1.3: distribution mean age (u020) different different 2011OAC supergroups Leicester?","code":""},{"path":"comparing-data.html","id":"correlation","chapter":"7 Comparing data","heading":"7.4 Correlation","text":"term correlation used refer series standardised measures covariance, can used statistically assess whether two variables related .Furthermore, two variables related, measures can identify whether :positively related:\nentities high values one tend high values ;\nentities low values one tend low values ;\nentities high values one tend high values ;entities low values one tend low values ;negatively:\nentities high values one tend low values ;\nentities low values one tend high values .\nentities high values one tend low values ;entities low values one tend high values .Correlation can calculated many ways, three approaches far common. start null hypothesis relationship variables. Thus, p-value pre-defined significance threshold, null hypothesis rejected, conclusion relationship two variables.test significant case:positive correlation value indicates positive relationship;negative correlation value indicates negative relationship;square correlation value can taken indication percentage shared variance two variables.However, one different assumptions variables’ distribution thus implements general ideal measure different way:two variables normally distributed:\nPearson’s r;\nPearson’s r;two variables normally distributed:\nties among values:\nSpearman’s rho;\n\nties among values:\nKendall’s tau.\n\nties among values:\nSpearman’s rho;\nSpearman’s rho;ties among values:\nKendall’s tau.\nKendall’s tau.","code":""},{"path":"comparing-data.html","id":"example-1","chapter":"7 Comparing data","heading":"7.4.1 Example","text":"studying people live cities, number questions might arise live move around city. instance, looking map Leicester, clear (many English cities) seems high concentration flats city centre. time, seems almost flats suburbs. might led us ask: “households living flats (thus mostly city centre) amount cars households living city center?”due many reasons. suburbs England largely residential, whereas working places located city centre. people living flats might likely walk cycle work, commute using public transportation within city cities. City centres usually afford less spaces parking. Many flats rented students, might less likely car. list continue, still hypothesis based certain (probably biased) view city. Can use data analysis explore whether ground hypothesis?dataset used create 2011 Output Area Classification (2011OAC) contains two variables might help explore issue. data current anymore, values might collect conduct fresh survey specific study. However, can still provide insight.u089: count flats per Output Area (OA). statistical unit variable Household_Spaces. OAs vary size composition, can use Total_Household_Spaces calculate percentage flats per OA, stable measure.\nperc_flats = (u089 / Total_Household_Spaces) * 100\nperc_flats = (u089 / Total_Household_Spaces) * 100u118: 2 cars vans household. statistical unit variable Household. OAs vary size composition, can use Total_Household_Spaces calculate percentage households per OA 2 cars vans, stable measure.\nperc_2ormore_cars = (u118 / Total_Households) * 100\nperc_2ormore_cars = (u118 / Total_Households) * 100The process transforming variables within certain range (percentage, thus using [0..100] range, [0..1] range) commonly referred normalisation. process transforming variable mean zero standard deviation one (z-scores) commonly referred standardisation. However, note terms sometime used interchangably.Plotting two variables together scatterplot reveals pattern. Indeed, low percentage households living flats two cars. However, proportion households owning two cars live suburbs seem span almost throughout whole range, zero 80%. seems indicate level negative relationship, picture clearly far less clear-cut might initially assumed. initial assumption car ownership households living flats seems hold, probably didn’t consider situation suburbs sufficient care.first step establishing whether relationship two variables assess whether normally distributed, thus correlation test use analysis. scatterplot already seem suggest variables rather skewed.969 OAs Leicester, can set significance threshold 0.01. results stats::shapiro.test functions show neither two variables normally distributed. Transforming variables using inverse hyperbolic sine still result normally distributed variables. Thus, discard Pearson’s r option explore correlation two variables.next step assess whether ties among values two variables. code fist counts number cases per value. counts number values number cases greater one.variable perc_flats 127 values ties perc_2ormore_cars 115 values ties. , using Spearman’s rho advisable Kendall’s tau used. , can set significance threshold 0.01.Finally, can run stats::cor.test function assess relationship two variables. code saves results test variable. afford subsequent actions. First, can show full results simply invoking name variable (term used programming-related meaning ) final line code. Second, can extract square estimate value RMarkdwon following paragraph, show percentage shared variace.percentage flats percentage households owning 2 cars vans per OA city Leicester negative related, relationship significant (p-value < 0.01) correlation value negative (tau = -0.41). two variables share 16.8% variance. can thus conclude significant weak relationship two variables.","code":"\nflats_and_cars <-\n  leicester_2011OAC %>%\n  dplyr::mutate(\n    perc_flats = (u089 / Total_Household_Spaces) * 100,\n    perc_2ormore_cars = (u118 / Total_Households) * 100\n  ) %>%\n  dplyr::select(\n    OA11CD, supgrpname, supgrpcode,\n    perc_flats, perc_2ormore_cars\n  )\nflats_and_cars %>%\n  dplyr::select(perc_flats, perc_2ormore_cars) %>%\n  dplyr::mutate(\n    ihs_perc_flats = asinh(perc_flats),\n    ihs_perc_2omcars = asinh(perc_2ormore_cars)\n  ) %>%\n  pastecs::stat.desc(basic = FALSE, desc = FALSE, norm = TRUE) %>%\n  knitr::kable()\nties_perc_flats <-\n  flats_and_cars %>%\n  dplyr::count(perc_flats) %>%\n  dplyr::filter(n > 1) %>% \n  # Specify wt = n() to count rows\n  # otherwise n is taken as weight\n  dplyr::count(wt = n()) %>%\n  dplyr::pull(n)\n\nties_perc_2ormore_cars <-\n  flats_and_cars %>%\n  dplyr::count(perc_2ormore_cars) %>%\n  dplyr::filter(n > 1) %>% \n  # Specify wt = n() to count rows\n  # otherwise n is taken as weight\n  dplyr::count(wt = n()) %>%\n  dplyr::pull(n)\nflats_and_cars_corKendall <-\n  flats_and_cars %$%\n  stats::cor.test(\n    perc_flats, perc_2ormore_cars, \n    method = \"kendall\"\n  )\n\nflats_and_cars_corKendall## \n##  Kendall's rank correlation tau\n## \n## data:  perc_flats and perc_2ormore_cars\n## z = -19.026, p-value < 2.2e-16\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##        tau \n## -0.4094335The percentage of flats and the percentage of households \nowning 2 or more cars or vans per OA in the city of Leicester \nare negative related, as the relationship is significant \n(`p-value < 0.01`) and the correlation value is negative \n(`tau =` -0.41). The two variables share 16.8% of variance. We can thus conclude \nthat there is significant but very weak relationship \nbetween the two variables."},{"path":"comparing-data.html","id":"exercise-314.2","chapter":"7 Comparing data","heading":"7.5 Exercise 314.2","text":"Question 314.2.1: mentioned , discussing movement cities, assumption people living city centre live flats work cycle work, whereas people living suburbs live whole houses commute via car. Study correlation presence flats (u089) people commuting work foot, bicycle similar means (u122) OAs. Consider whether values might need normalised otherwised transformed starting testing procedure.Question 314.2.2: Another interesting issue explore relationship car ownership use public transport. Study correlation presence households owning 2 cars vans (u118) people commuting work via public transport (u120) foot, bicycle similar means (u122) OAs. Consider whether values might need normalised otherwised transformed starting testing procedure.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"regression-analysis.html","id":"regression-analysis","chapter":"8 Regression analysis","heading":"8 Regression analysis","text":"","code":""},{"path":"regression-analysis.html","id":"simple-regression","chapter":"8 Regression analysis","heading":"8.1 Simple regression","text":"simple regression analysis supervised machine learning approach creating model able predict value one outcome variable \\(Y\\) based one predictor variable \\(X_1\\), estimating intercept \\(b_0\\) coefficient (slope) \\(b_1\\), accounting reasonable amount error \\(\\epsilon\\).\\[Y_i = (b_0 + b_1 * X_{i1}) + \\epsilon_i \\]Least squares commonly used approach generate regression model. model fits line minimise squared values residuals (errors), calculated squared difference observed values values predicted model.\\[redidual = \\sum(observed - model)^2\\]model considered robust residuals show particular trends, indicate “something” interfering model. particular, assumption regression model :linearity: relationship actually linear;normality residuals: standard residuals normally distributed mean 0;homoscedasticity residuals: level predictor variable(s) variance standard residuals (homo-scedasticity) rather different (hetero-scedasticity);independence residuals: adjacent standard residuals correlated.","code":""},{"path":"regression-analysis.html","id":"example-2","chapter":"8 Regression analysis","heading":"8.1.1 Example","text":"example seen lecture illustrated simple regression can used create model predict arrival delay based departure delay flight, based data available nycflights13 dataset flight November 20th, 2013. scatterplot seems indicate relationship indeed linear.\\[arr\\_delay_i = (Intercept + Coefficient_{dep\\_delay} * dep\\_delay_{i1}) + \\epsilon_i \\]code generates model using function lm, function summary obtain summary results test. model summary saved variables delay_model delay_model_summary, respectively, use . variable delay_model_summary can called directly visualise result test.image highlights important values output: adjusted \\(R^2\\) value; model significance value p-value related F-statistic information F-statistic; intercept dep_delay coefficient estimates Estimate column related significance values column Pr(>|t|).output indicates:p-value: < 2.2e-16: \\(p<.001\\) model significant;\nderived comparing calulated F-statistic value F distribution 3396.74 specified degrees freedom (1, 972);\nReport : \\(F(1, 972) = 3396.74\\)\nderived comparing calulated F-statistic value F distribution 3396.74 specified degrees freedom (1, 972);Report : \\(F(1, 972) = 3396.74\\)Adjusted R-squared: 0.7773: departure delay can account 77.73% arrival delay;Coefficients:\nIntercept estimate -4.9672 significant;\ndep_delay coefficient (slope) estimate 1.0423 significant.\nIntercept estimate -4.9672 significant;dep_delay coefficient (slope) estimate 1.0423 significant.","code":"\n# Load the library\nlibrary(nycflights13)\n\n# November 20th, 2013\nflights_nov_20 <- nycflights13::flights %>%\n  dplyr::filter(!is.na(dep_delay), !is.na(arr_delay), month == 11, day ==20) \n# Classic R coding version\n# delay_model <- lm(arr_delay ~ dep_delay, data = flights_nov_20)\n# delay_model_summary <- summary(delay_model)\n\n# Load magrittr library to use %$%\nlibrary(magrittr)## \n## Attaching package: 'magrittr'## The following object is masked from 'package:purrr':\n## \n##     set_names## The following object is masked from 'package:tidyr':\n## \n##     extract\ndelay_model <- flights_nov_20 %$%\n  lm(arr_delay ~ dep_delay) \n\ndelay_model_summary <- delay_model %>%\n  summary()\n\ndelay_model_summary## \n## Call:\n## lm(formula = arr_delay ~ dep_delay)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -43.906  -9.022  -1.758   8.678  57.052 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -4.96717    0.43748  -11.35   <2e-16 ***\n## dep_delay    1.04229    0.01788   58.28   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.62 on 972 degrees of freedom\n## Multiple R-squared:  0.7775, Adjusted R-squared:  0.7773 \n## F-statistic:  3397 on 1 and 972 DF,  p-value: < 2.2e-16\nflights_nov_20 %>%\n  ggplot2::ggplot(aes(x = dep_delay, y = arr_delay)) +\n  ggplot2::geom_point() + ggplot2::coord_fixed(ratio = 1) +\n  ggplot2::geom_abline(intercept = 4.0943, slope = 1.04229, color=\"red\")"},{"path":"regression-analysis.html","id":"checking-assumptions","chapter":"8 Regression analysis","heading":"8.1.2 Checking assumptions","text":"","code":""},{"path":"regression-analysis.html","id":"normality","chapter":"8 Regression analysis","heading":"8.1.2.1 Normality","text":"Shapiro-Wilk test can used check normality standard residuals. test significant robust models. example , standard residuals normally distributed. However, plot show distribution residuals far away normal distribution.","code":"\ndelay_model %>% \n  stats::rstandard() %>% \n  stats::shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.98231, p-value = 1.73e-09"},{"path":"regression-analysis.html","id":"homoscedasticity","chapter":"8 Regression analysis","heading":"8.1.2.2 Homoscedasticity","text":"Breusch-Pagan test can used check homoscedasticity standard residuals. test significant robust models. example , standard residuals homoscedastic.","code":"\nlibrary(lmtest)\n\ndelay_model %>% \n  lmtest::bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 0.017316, df = 1, p-value = 0.8953"},{"path":"regression-analysis.html","id":"independence","chapter":"8 Regression analysis","heading":"8.1.2.3 Independence","text":"Durbin-Watson test can used check independence residuals. test statistic close 2 (1 3) significant robust models. example , standard residuals might completely independent. Note, however, result depends order data.","code":"\n# Also part of the library lmtest\ndelay_model %>%\n  lmtest::dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 1.8731, p-value = 0.02358\n## alternative hypothesis: true autocorrelation is greater than 0"},{"path":"regression-analysis.html","id":"plots","chapter":"8 Regression analysis","heading":"8.1.2.4 Plots","text":"plot.lm function can used explore residuals visuallly. Usage illustrated . Residuals vs Fitted Scale-Location plot provide insight homoscedasticity residuals, Normal Q-Q plot provides illustration normality residuals, Residuals vs Leverage can useful identify exceptional cases (e.g., Cook’s distance greater 1).","code":"\ndelay_model %>%\n  plot(which = c(1))\ndelay_model %>%\n  plot(which = c(2))\ndelay_model %>%\n  plot(which = c(3))\ndelay_model %>%\n  plot(which = c(5))"},{"path":"regression-analysis.html","id":"how-to-report","chapter":"8 Regression analysis","heading":"8.1.3 How to report","text":"Overall, can say delay model computed fit (\\(F(1, 972) = 3396.74\\), \\(p < .001\\)), indicating departure delay might account 77.73% arrival delay. However model partially robust. residuals satisfy homoscedasticity assumption (Breusch-Pagan test, \\(BP = 0.02\\), \\(p =0.9\\)), independence assumption (Durbin-Watson test, \\(DW = 1.87\\), \\(p =0.02\\)), normally distributed (Shapiro-Wilk test, \\(W = 0.98\\), \\(p < .001\\)).stargazer function stargazer library can applied model delay_model generate nicer output RMarkdown PDF documents including results = \"asis\" R snippet option.","code":"\n# Install stargazer if not yet installed\n# install.packages(\"stargazer\")\n\nlibrary(stargazer)\n\n# Not rendered in bookdown\nstargazer(delay_model, header = FALSE)"},{"path":"regression-analysis.html","id":"multiple-regression","chapter":"8 Regression analysis","heading":"8.2 Multiple regression","text":"multiple regression analysis supervised machine learning approach creating model able predict value one outcome variable \\(Y\\) based two predictor variables \\(X_1 \\dots X_M\\), estimating intercept \\(b_0\\) coefficients (slopes) \\(b_1 \\dots b_M\\), accounting reasonable amount error \\(\\epsilon\\).\\[Y_i = (b_0 + b_1 * X_{i1} + b_2 * X_{i2} + \\dots + b_M * X_{iM}) + \\epsilon_i \\]assumptions simple regression, plus assumption multicollinearity: two predictor variables used model, pair variables correlated. assumption can tested checking variance inflation factor (VIF). largest VIF value greater 10 average VIF substantially greater 1, might issue multicollinearity.","code":""},{"path":"regression-analysis.html","id":"example-3","chapter":"8 Regression analysis","heading":"8.2.1 Example","text":"example explores whether regression model can created estimate number people Leicester commuting work using private transport (u121) Leicester, using number people different industry sectors predictors.instance, occupations electricity, gas, steam air conditioning supply (u144) require travel distances equipment, thus related variable u144 included model, whereas people working information communication might likely work home commute public transport.multiple regression model can specified similar way simple regression model, using lm function, adding additional predictor variables using + operator.output suggests model fit (\\(F(4, 964) = 150.62\\), \\(p < .001\\)), indicating model based presence people working four selected industry sectors can account 38.21% number people using private transportation commute work. However model partially robust. residuals normally distributed (Shapiro-Wilk test, \\(W = 1\\), \\(p =0.83\\)) seems multicollinearity average VIF \\(1.02\\), residuals don’t satisfy homoscedasticity assumption (Breusch-Pagan test, \\(BP = 28.4\\), \\(p < .001\\)), independence assumption (Durbin-Watson test, \\(DW = 1.84\\), \\(p < .01\\)).coefficient values calculated lm functions important create model, provide useful information. instance, coefficient variable perc_u144 1.169, indicates presence people working electricity, gas, steam air conditioning supply increases one percentage point, number people using private transportation commute work increases 1.169 percentage points, according model. coefficients also indicate presence people working accommodation food service activities actually negative impact (context variables selected model) number people using private transportation commute work.example, variables use unit similar type, makes interpreting model relatively simple. case, can useful look standardized \\(\\beta\\), provide information measured terms standard deviation, make comparisons variables different types easier draw. instance, values calculated using function lm.beta library lm.beta indicate presence people working construction highest impact outcome varaible. presence people working construction increases one standard deviation, number people using private transportation commute work increases 0.29 standard deviations, according model.","code":"\nleicester_2011OAC <- readr::read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n# Select and \n# normalise variables\nleicester_2011OAC_transp <-\n  leicester_2011OAC %>%\n  dplyr::select(\n    OA11CD, \n    Total_Pop_No_NI_Students_16_to_74, Total_Employment_16_to_74, \n    u121, u141:u158\n  ) %>%\n  # percentage method of travel\n  dplyr::mutate(\n    u121 = (u121 / Total_Pop_No_NI_Students_16_to_74) * 100\n  ) %>%\n  # percentage across industry sector columns\n  dplyr::mutate(\n    dplyr::across( \n      u141:u158,\n      function(x){ (x / Total_Employment_16_to_74) * 100 }\n    )\n  ) %>%\n  # rename columns\n  dplyr::rename_with(\n    function(x){ paste0(\"perc_\", x) },\n    c(u121, u141:u158)\n  )\n\n# Selected variables\n\n# perc_u120: Method of Travel to Work, Private Transport\n# perc_u142: Industry Sector, Mining and quarrying\n# perc_u144: Industry Sector, Electricity, gas, steam and air conditioning ...\n# perc_u146: Industry Sector, Construction\n# perc_u149: Industry Sector, Accommodation and food service activities\n\n# Create model\ncommuting_model1 <- \n  leicester_2011OAC_transp %$%\n  lm(\n    perc_u121 ~ \n      perc_u142 + perc_u144 + perc_u146 + perc_u149\n  )\n\n# Print summary\ncommuting_model1 %>%\n  summary()## \n## Call:\n## lm(formula = perc_u121 ~ perc_u142 + perc_u144 + perc_u146 + \n##     perc_u149)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -35.315  -6.598  -0.244   6.439  31.472 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 37.12690    0.94148  39.434  < 2e-16 ***\n## perc_u142    3.74768    1.21255   3.091  0.00205 ** \n## perc_u144    1.16865    0.25328   4.614 4.48e-06 ***\n## perc_u146    1.05408    0.09335  11.291  < 2e-16 ***\n## perc_u149   -1.56948    0.08435 -18.606  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.481 on 964 degrees of freedom\n## Multiple R-squared:  0.3846, Adjusted R-squared:  0.3821 \n## F-statistic: 150.6 on 4 and 964 DF,  p-value: < 2.2e-16\n# Not rendered in bookdown\nstargazer(commuting_model1, header=FALSE)\ncommuting_model1 %>%\n  rstandard() %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.99889, p-value = 0.8307\ncommuting_model1 %>% \n  bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 28.403, df = 4, p-value = 1.033e-05\ncommuting_model1 %>%\n  dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 1.835, p-value = 0.004908\n## alternative hypothesis: true autocorrelation is greater than 0\nlibrary(car)\n\ncommuting_model1 %>%\n  vif()## perc_u142 perc_u144 perc_u146 perc_u149 \n##  1.006906  1.016578  1.037422  1.035663\n# Install lm.beta library if necessary\n# install.packages(\"lm.beta\")\nlibrary(lm.beta)\n\nlm.beta(commuting_model1)## \n## Call:\n## lm(formula = perc_u121 ~ perc_u142 + perc_u144 + perc_u146 + \n##     perc_u149)\n## \n## Standardized Coefficients::\n## (Intercept)   perc_u142   perc_u144   perc_u146   perc_u149 \n##  0.00000000  0.07836017  0.11754058  0.29057993 -0.47841083"},{"path":"regression-analysis.html","id":"exercise-324.1","chapter":"8 Regression analysis","heading":"8.3 Exercise 324.1","text":"Question 324.1.1: Create model outcome variable presence people using private transport commuting work, using stepwise “” approach, variables created example related presence people working different industry sectors (perc_u141 perc_u158) scope.Question 324.1.2: presence people using public transportation commute work statistically, linearly related mean age (u020)?Question 324.1.3: presence people using public transportation commute work statistically, linearly related (subset ) age structure categories (u007 u019)?Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"supervised-machine-learning.html","id":"supervised-machine-learning","chapter":"9 Supervised machine learning","heading":"9 Supervised machine learning","text":"","code":""},{"path":"supervised-machine-learning.html","id":"introduction-2","chapter":"9 Supervised machine learning","heading":"9.1 Introduction","text":"field machine learning sits intersection computer science statistics, core component data science. According Mitchell (1997), “field machine learning concerned question construct computer programs automatically improve experience.”Machine learning approaches divided two main types.Supervised:\ntraining “predictive” model data;\none () attribute dataset used “predict” another attribute.\ntraining “predictive” model data;one () attribute dataset used “predict” another attribute.Unsupervised:\ndiscovery descriptive patterns data;\ncommonly used data mining.\ndiscovery descriptive patterns data;commonly used data mining.Classification one classic supervised machine learning tasks, algorithms used learn (.e., model) relationship series input values (.k.. predictors, independent variables) output categorical values labels (.k.. outcome, dependent variable). model trained training dataset can learn relationship input labels, used label new, unlabeled data.","code":""},{"path":"supervised-machine-learning.html","id":"artificial-neural-networks","chapter":"9 Supervised machine learning","heading":"9.1.1 Artificial neural networks","text":"Artificial neural networks (ANNs) one studied approaches supervised machine learning, term actually defines large set different approaches. model aims simulate simplistic version brain made artificial neurons. artificial neuron combines series input values one output values, using series weights (one per input value) activation function. aim model learn optimal set weights , combined input values, generates correct output value. latter also influenced activation function, modulates final result.neuron effectively regression model. input values predictors (independent variables), output outcome (dependent variable), weights coefficients (see also previous practical regression models). selection activation function defines regression model. ANNs commonly used classification, one common activation functions used sigmoid, thus rendering every single neuron logistic regression.instance ANN defined topology (number layers nodes), activation functions algorithm used train network. selection parameters renders construction ANNs complex task, quality result frequently relies experience data scientist.Number layers\nSingle-layer network: one node input variable one node per category output variable, effectively logistic regression.\nMulti-layer network: adds one hidden layer, aims capture hidden “features” data, combinations input values, use final classification.\nDeep neural networks: several hidden layers, aiming capture complex “features” data.\nSingle-layer network: one node input variable one node per category output variable, effectively logistic regression.Multi-layer network: adds one hidden layer, aims capture hidden “features” data, combinations input values, use final classification.Deep neural networks: several hidden layers, aiming capture complex “features” data.Number nodes\nnumber nodes needs selected one hidden layers.\nnumber nodes needs selected one hidden layers.","code":""},{"path":"supervised-machine-learning.html","id":"support-vector-machines","chapter":"9 Supervised machine learning","heading":"9.1.2 Support vector machines","text":"Support vector machines (SVMs) another common approach supervised classification. SVMs perform classification task partitioning data space regions separated hyperplanes. instance, bi-dimensional space, hyperplane line, algorithm designed find line best separates two groups data. Computationally, process dissimilar linear regression.","code":""},{"path":"supervised-machine-learning.html","id":"confusion-matrices","chapter":"9 Supervised machine learning","heading":"9.1.3 Confusion matrices","text":"classification model created, next step validation. latter can involve different approaches procedures, one common simple approaches split data training testing set. model trained training set validated using testing set. sets contain input values (predictors) output values (outcome).model trained using training set can used predict values testing set. outcome prediction can compared actual categories testing dataset. confusion matrix representation correspondence actual values predicted values testing dataset, including:true positive: correctly classified first (positive) class;true negative: correctly classified second (negative) class;false positive: incorrectly classified first (positive) class;false negative: incorrectly classified second (negative) class.number true false positive negatives used calculate number performance measures. simplest measures performance accuracy error rate.\\[\naccuracy = \\frac{true\\ positive + true\\ negative}{total\\ number\\ \\ cases}\n\\]\n\\[\nerror\\ rate = 1 - accuracy\n\\]also number additional measures can provide insight quality prediction, sensitivity (true positive rate) specificity (true negative rate). model created predict binary categorical variable, based definition (first category positive, second category negative), sensitivity measure quality predicting first category, specificity measure quality predicting second category.\\[\nsensitivity = \\frac{true\\ positive}{true\\ positive + false\\ negative} = \\frac{correct\\ 1st}{\\ 1st}\n\\]\\[\nspecificity = \\frac{true\\ negative}{true\\ negative + false\\ positive} = \\frac{correct\\ 2nd}{\\ 2nd}\n\\]Two , similar measures precision recall. model high precision model can trusted make correct prediction identifying observation part first category. formula recall used sensitivity, case, different interpretation, derived computer science literature search engines, model high recall able correctly retrieve items specified category. Note precision recall dependent one two categories defined first.\\[\nprecision = \\frac{true\\ positive}{true\\ positive + false\\ positive} = \\frac{correct\\ 1st}{predicted\\ \\ 1st}\n\\]\\[\nrecall = \\frac{true\\ positive}{true\\ positive + false\\ negative} = \\frac{correct\\ 1st}{\\ 1st}\n\\]\nPrecision recall can also combined single measure performance called F-score (.k.., F-measure F1).\\[\nF-score = \\frac{2 \\times precision \\times recall}{precision + recall}\n\\]\nFinally, kappa statistic (common Cohen’s kappa) additional measure accuracy, measures agreement prediction actual values, also accounting probability correct prediction chance.","code":""},{"path":"supervised-machine-learning.html","id":"examples","chapter":"9 Supervised machine learning","heading":"9.2 Examples","text":"two examples explore relation variables United Kingdom 2011 Census included among 167 initial variables used create 2011 Output Area Classification (Gale et al., 2016) Rural Urban Classification (2011) Output Areas England Wales created Office National Statistics. various examples models explore whether possible learn rural-urban distinction using census variables, Local Authority Districts (LADs) Leicestershire (excluding city Leicester .code uses libraries caret, e1071 neuralnet. Please install continuing practical.","code":"\ninstall.packages(\"caret\")\ninstall.packages(\"e1071\")\ninstall.packages(\"neuralnet\")"},{"path":"supervised-machine-learning.html","id":"data","chapter":"9 Supervised machine learning","heading":"9.2.1 Data","text":"examples use data seen previous practicals, 7 LADs Leicestershire outside boundaries city Leicester: Blaby, Charnwood, Harborough, Hinckley Bosworth, Melton, North West Leicestershire, Oadby Wigston. data loaded 2011_OAC_Raw_uVariables_Leicestershire.csv. second part code extracts data Rural Urban Classification (2011) compressed file RUC11_OA11_EW.zip, loads extracted data finally deletes .can join two datasets create simplified, binary rural - urban classification, used examples .","code":"\n# Libraries\nlibrary(tidyverse)\nlibrary(magrittr)\n\n# 2011 OAC data for Leicestershire (excl. Leicester)\nliec_shire_2011OAC <- readr::read_csv(\"2011_OAC_Raw_uVariables_Leicestershire.csv\")\n\n# Rural Urban Classification (2011)\n# >>> Note that if you upload the file to RStudio Server\n# >>> the file will be automatically unzipped\n# >>> thus the unzip and unlink instrcutions are not necessary\nunzip(\"RUC11_OA11_EW.zip\")\nru_class_2011 <- readr::read_csv(\"RUC11_OA11_EW.csv\")\nunlink(\"RUC11_OA11_EW.csv\")\nliec_shire_2011OAC_RU <-\n  liec_shire_2011OAC %>%\n  dplyr::left_join(ru_class_2011) %>%\n  dplyr::mutate(\n    rural_urban = \n      forcats::fct_recode(\n        RUC11CD,\n        urban = \"C1\",\n        rural = \"D1\",\n        rural = \"E1\",\n        rural = \"F1\"\n      ) %>% \n      forcats::fct_relevel(\n        c(\"rural\", \"urban\")\n      )\n  )"},{"path":"supervised-machine-learning.html","id":"logistic-regression","chapter":"9 Supervised machine learning","heading":"9.2.2 Logistic regression","text":"Can predict whether Output Area (OA) urban rural, solely based population density?two patters plot seem quite close even plotted using logarithmically transformed x-axis. first step, can extract dataset data need, create logarithmic transformation population density value. able perform simple validatin model, can divide data training (80% dataset) testing set (20% dataset).can compute logit model using stats::glm function specifying binomial() family. summary model highlights model significant, Residual deviance fairly close Null deviance (null model), good sign.per regression models, necessary test assumptions logit model overall distribution residuals. However, model one predictor, confine performance analysis simple validation.can test performance model validation exercise, using testing dataset. Finally, can compare results prediction original data using confusion matrix.","code":"\nliec_shire_2011OAC_RU %>%\n  ggplot2::ggplot(\n    aes(\n      x = u006, \n      y = rural_urban\n    )\n  ) +\n  ggplot2::geom_point(\n    aes(\n      color = rural_urban,\n      shape = rural_urban\n    )\n  ) +\n  ggplot2::scale_color_manual(values = c(\"deepskyblue2\", \"darkgreen\")) +\n  ggplot2::scale_x_log10() +\n  ggplot2::theme_bw()\n# Data for logit model\nru_logit_data <-\n  liec_shire_2011OAC_RU %>%\n  dplyr::select(OA11CD, u006, rural_urban) %>%\n  dplyr::mutate(\n    density_log = log10(u006)\n  )\n\n# Training set\nru_logit_data_trainig <-\n  ru_logit_data %>% \n  slice_sample(prop = 0.8)\n\n# Testing set\nru_logit_data_testing <- \n  ru_logit_data %>% \n  anti_join(ru_logit_data_trainig)\nru_logit_model <- \n  ru_logit_data_trainig %$%\n  stats::glm(\n    rural_urban ~ \n      density_log, \n    family = binomial()\n  )\n\nru_logit_model %>%  \n  summary()## \n## Call:\n## stats::glm(formula = rural_urban ~ density_log, family = binomial())\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1978  -0.5532   0.5249   0.6490   2.1024  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -1.3623     0.1333  -10.22   <2e-16 ***\n## density_log   1.8729     0.1016   18.44   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2071.5  on 1667  degrees of freedom\n## Residual deviance: 1578.0  on 1666  degrees of freedom\n## AIC: 1582\n## \n## Number of Fisher Scoring iterations: 4\nru_logit_prediction <- \n  ru_logit_model %>%\n  # Use model to predict values\n  stats::predict(\n    ru_logit_data_testing, \n    type = \"response\"\n  ) %>%\n  as.numeric()\n\nru_logit_data_testing <- \n  ru_logit_data_testing %>%\n  tibble::add_column(\n    # Add column with predicted class\n    logit_predicted_ru = \n      # Values below 0.5 indicate first factor level (rural)\n      # Values above 0.5 indicate second factor level (ruban)\n      ifelse(\n        ru_logit_prediction <= 0.5,\n        \"rural\", # first factor level\n        \"urban\"  # second factor level\n      ) %>%\n      forcats::as_factor() %>% \n      forcats::fct_relevel(\n        c(\"rural\", \"urban\")\n      )\n  )\n\n# Load library for confusion matrix\nlibrary(caret)\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_logit_data_testing %>% dplyr::pull(logit_predicted_ru),\n  ru_logit_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n)## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    66    36\n##      urban    59   256\n##                                           \n##                Accuracy : 0.7722          \n##                  95% CI : (0.7289, 0.8116)\n##     No Information Rate : 0.7002          \n##     P-Value [Acc > NIR] : 0.0006239       \n##                                           \n##                   Kappa : 0.4272          \n##                                           \n##  Mcnemar's Test P-Value : 0.0239986       \n##                                           \n##             Sensitivity : 0.5280          \n##             Specificity : 0.8767          \n##          Pos Pred Value : 0.6471          \n##          Neg Pred Value : 0.8127          \n##               Precision : 0.6471          \n##                  Recall : 0.5280          \n##                      F1 : 0.5815          \n##              Prevalence : 0.2998          \n##          Detection Rate : 0.1583          \n##    Detection Prevalence : 0.2446          \n##       Balanced Accuracy : 0.7024          \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"support-vector-machines-1","chapter":"9 Supervised machine learning","heading":"9.2.3 Support vector machines","text":"showcase use SVMs, example expands one building model urban - rural classification uses total population area (logarithmically transformed) two separate input values, rather combined population density. aim SVM find line maximises margin two groups shown plot .plot illustrates two variables skewed (note axes logarithmically transformed) two groups linearly separable. can thus follow procedure similar one seen : extract necessary data; split data training testing validation; build model; predict values testing set interpret confusion matrix.third complex example, can explore build model rural - urban classification using presence five different types dwelling input variables. confusion matrix clearly illustrates simple linear SVM adequate construct model.","code":"\nliec_shire_2011OAC_RU %>%\n  ggplot2::ggplot(\n    aes(\n      x = u006, \n      y = Total_Population\n    )\n  ) +\n  ggplot2::geom_point(\n    aes(\n      color = rural_urban,\n      shape = rural_urban\n    )\n  ) +\n  ggplot2::scale_color_manual(values = c(\"deepskyblue2\", \"darkgreen\")) +\n  ggplot2::scale_x_log10() +\n  ggplot2::scale_y_log10() +\n  ggplot2::theme_bw()\n# Data for SVM model\nru_svm_data <-\n  liec_shire_2011OAC_RU %>%\n  dplyr::select(OA11CD, Total_Population, u006, rural_urban) %>%\n  dplyr::mutate(\n    area_log = log10(u006),\n    population_log = log10(Total_Population)\n  )\n\n# Training set\nru_svm_data_trainig <-\n  ru_svm_data %>% \n  slice_sample(prop = 0.8)\n\n# Testing set\nru_svm_data_testing <- \n  ru_svm_data %>% \n  anti_join(ru_svm_data_trainig)\n\n# Load library for svm function\nlibrary(e1071)\n\n# Build the model\nru_svm_model <- \n  ru_svm_data_trainig %$%\n  e1071::svm(\n    rural_urban ~ \n      area_log + population_log, \n    # Use a simple linear hyperplane\n    kernel = \"linear\", \n    # Scale the data\n    scale = TRUE,\n    # Cost value for observations\n    # crossing the hyperplane\n    cost = 10\n  )\n\n# Predict the values for the testing dataset\nru_svm_prediction <-\n  stats::predict(\n    ru_svm_model,\n    ru_svm_data_testing %>% \n      dplyr::select(area_log, population_log)\n  )\n\n# Add predicted values to the table\nru_svm_data_testing <-\n  ru_svm_data_testing %>%\n  tibble::add_column(\n    svm_predicted_ru = ru_svm_prediction\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_svm_data_testing %>% dplyr::pull(svm_predicted_ru),\n  ru_svm_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    56    24\n##      urban    67   270\n##                                          \n##                Accuracy : 0.7818         \n##                  95% CI : (0.739, 0.8205)\n##     No Information Rate : 0.705          \n##     P-Value [Acc > NIR] : 0.0002571      \n##                                          \n##                   Kappa : 0.4159         \n##                                          \n##  Mcnemar's Test P-Value : 1.069e-05      \n##                                          \n##             Sensitivity : 0.4553         \n##             Specificity : 0.9184         \n##          Pos Pred Value : 0.7000         \n##          Neg Pred Value : 0.8012         \n##               Precision : 0.7000         \n##                  Recall : 0.4553         \n##                      F1 : 0.5517         \n##              Prevalence : 0.2950         \n##          Detection Rate : 0.1343         \n##    Detection Prevalence : 0.1918         \n##       Balanced Accuracy : 0.6868         \n##                                          \n##        'Positive' Class : rural          \n## \n# Data for SVM model\nru_dwellings_data <-\n  liec_shire_2011OAC_RU %>%\n  dplyr::select(\n    OA11CD, rural_urban, Total_Dwellings,\n    u086:u090\n  ) %>%\n  # scale across\n  dplyr::mutate(\n    dplyr::across( \n      u086:u090,\n      scale\n      #function(x){ (x / Total_Dwellings) * 100 }\n    )\n  ) %>%\n  dplyr::rename(\n    scaled_detached = u086,\n    scaled_semidetached = u087,\n    scaled_terraced = u088,\n    scaled_flats = u089,    \n    scaled_carava_tmp = u090\n  ) %>%\n  dplyr::select(-Total_Dwellings)\n\n# Training set\nru_dwellings_data_trainig <-\n  ru_dwellings_data %>% \n  slice_sample(prop = 0.8)\n\n# Testing set\nru_dwellings_data_testing <- \n  ru_dwellings_data %>% \n  anti_join(ru_dwellings_data_trainig)\n\n# Build the model\nru_dwellings_svm_model <- \n  ru_dwellings_data_trainig %$%\n  e1071::svm(\n    rural_urban ~ \n      scaled_detached + scaled_semidetached + scaled_terraced + \n      scaled_flats + scaled_carava_tmp, \n    # Use a simple linear hyperplane\n    kernel = \"linear\",\n    # Cost value for observations\n    # crossing the hyperplane\n    cost = 10\n  )\n\n# Predict the values for the testing dataset\nru_dwellings_svm_prediction <-\n  stats::predict(\n    ru_dwellings_svm_model,\n    ru_dwellings_data_testing %>% \n      dplyr::select(scaled_detached:scaled_carava_tmp)\n  )\n\n# Add predicted values to the table\nru_dwellings_data_testing <-\n  ru_dwellings_data_testing %>%\n  tibble::add_column(\n    dwellings_svm_predicted_ru = ru_dwellings_svm_prediction\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_dwellings_data_testing %>% dplyr::pull(dwellings_svm_predicted_ru),\n  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural     0     0\n##      urban   144   273\n##                                           \n##                Accuracy : 0.6547          \n##                  95% CI : (0.6069, 0.7003)\n##     No Information Rate : 0.6547          \n##     P-Value [Acc > NIR] : 0.5226          \n##                                           \n##                   Kappa : 0               \n##                                           \n##  Mcnemar's Test P-Value : <2e-16          \n##                                           \n##             Sensitivity : 0.0000          \n##             Specificity : 1.0000          \n##          Pos Pred Value :    NaN          \n##          Neg Pred Value : 0.6547          \n##               Precision :     NA          \n##                  Recall : 0.0000          \n##                      F1 :     NA          \n##              Prevalence : 0.3453          \n##          Detection Rate : 0.0000          \n##    Detection Prevalence : 0.0000          \n##       Balanced Accuracy : 0.5000          \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"kernels","chapter":"9 Supervised machine learning","heading":"9.2.4 Kernels","text":"Instead relying simple linear hyperplanes, can use “kernel trick” project data higher-dimensional space, might allow groups (easily) linearly separable.","code":"\n# Build a second model\n# using a radial kernel\nru_dwellings_svm_radial_model <- \n  ru_dwellings_data_trainig %$%\n  e1071::svm(\n    rural_urban ~ \n      scaled_detached + scaled_semidetached + scaled_terraced + \n      scaled_flats + scaled_carava_tmp, \n    # Use a radial kernel\n    kernel = \"radial\",\n    # Cost value for observations\n    # crossing the hyperplane\n    cost = 10\n  )\n\n# Predict the values for the testing dataset\nru_svm_dwellings_radial_prediction <-\n  stats::predict(\n    ru_dwellings_svm_radial_model,\n    ru_dwellings_data_testing %>% \n      dplyr::select(scaled_detached:scaled_carava_tmp)\n  )\n\n# Add predicted values to the table\nru_dwellings_data_testing <-\n  ru_dwellings_data_testing %>%\n  tibble::add_column(\n    dwellings_radial_predicted_ru = ru_svm_dwellings_radial_prediction\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_dwellings_data_testing %>% dplyr::pull(dwellings_radial_predicted_ru),\n  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    38    21\n##      urban   106   252\n##                                           \n##                Accuracy : 0.6954          \n##                  95% CI : (0.6488, 0.7393)\n##     No Information Rate : 0.6547          \n##     P-Value [Acc > NIR] : 0.0436          \n##                                           \n##                   Kappa : 0.2173          \n##                                           \n##  Mcnemar's Test P-Value : 9.069e-14       \n##                                           \n##             Sensitivity : 0.26389         \n##             Specificity : 0.92308         \n##          Pos Pred Value : 0.64407         \n##          Neg Pred Value : 0.70391         \n##               Precision : 0.64407         \n##                  Recall : 0.26389         \n##                      F1 : 0.37438         \n##              Prevalence : 0.34532         \n##          Detection Rate : 0.09113         \n##    Detection Prevalence : 0.14149         \n##       Balanced Accuracy : 0.59348         \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"artificial-neural-network","chapter":"9 Supervised machine learning","heading":"9.2.5 Artificial neural network","text":"Finally, can explore construction ANN input output variables seen previous example, compare one two approaches produces better results. example creates multi-layer ANN, using two hidden layers, five two nodes, respectively.","code":"\n# Load library for ANNs\nlibrary(neuralnet)\n\n# Build a third model\n# using an ANN\nru_dwellings_nnet_model <-\n  neuralnet::neuralnet(\n    rural_urban ~ \n      scaled_detached + scaled_semidetached + scaled_terraced + \n      scaled_flats + scaled_carava_tmp,\n    data = ru_dwellings_data_trainig,\n    # Use 2 hidden layers\n    hidden = c(5, 2),\n    # Max num of steps for training\n    stepmax = 1000000\n  )\nru_dwellings_nnet_model %>%  plot(rep = \"best\")\n# Predict the values for the testing dataset\nru_dwellings_nnet_prediction <-\n  neuralnet::compute(\n    ru_dwellings_nnet_model,\n    ru_dwellings_data_testing %>% \n      dplyr::select(scaled_detached:scaled_carava_tmp)\n  )\n\n# Derive predicted categories\nru_dwellings_nnet_predicted_categories <-\n  # from the prediction object\n  ru_dwellings_nnet_prediction %$%\n  # extract the result\n  # which is a matrix of probabilities\n  # for each object and category\n  net.result %>%\n  # select the column (thus the category)\n  # with higher probability\n  max.col %>%\n  # recode columns values as\n  # rural or urban\n  dplyr::recode(\n    `1` = \"rural\",\n    `2` = \"urban\"\n  ) %>%\n  forcats::as_factor() %>% \n  forcats::fct_relevel(\n    c(\"rural\", \"urban\")\n  )\n\n# Add predicted values to the table\nru_dwellings_data_testing <-\n  ru_dwellings_data_testing %>%\n  tibble::add_column(\n    dwellings_nnet_predicted_ru = \n      ru_dwellings_nnet_predicted_categories\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_dwellings_data_testing %>% dplyr::pull(dwellings_nnet_predicted_ru),\n  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    53    28\n##      urban    91   245\n##                                           \n##                Accuracy : 0.7146          \n##                  95% CI : (0.6687, 0.7575)\n##     No Information Rate : 0.6547          \n##     P-Value [Acc > NIR] : 0.005296        \n##                                           \n##                   Kappa : 0.2961          \n##                                           \n##  Mcnemar's Test P-Value : 1.319e-08       \n##                                           \n##             Sensitivity : 0.3681          \n##             Specificity : 0.8974          \n##          Pos Pred Value : 0.6543          \n##          Neg Pred Value : 0.7292          \n##               Precision : 0.6543          \n##                  Recall : 0.3681          \n##                      F1 : 0.4711          \n##              Prevalence : 0.3453          \n##          Detection Rate : 0.1271          \n##    Detection Prevalence : 0.1942          \n##       Balanced Accuracy : 0.6327          \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"exercise-404.1","chapter":"9 Supervised machine learning","heading":"9.3 Exercise 404.1","text":"Question 404.1.1: Create SVM model capable classifying areas Leicester Leicestershire rural urban based series variables relate “Economic Activity” among 167 initial variables used create 2011 Output Area Classification (Gale et al., 2016).Question 404.1.2: Create ANN using input output values used Question 404.1.1.Question 404.1.3: Assess one two models preforms better classification.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"unsupervised-machine-learning.html","id":"unsupervised-machine-learning","chapter":"10 Unsupervised machine learning","heading":"10 Unsupervised machine learning","text":"","code":""},{"path":"unsupervised-machine-learning.html","id":"introduction-3","chapter":"10 Unsupervised machine learning","heading":"10.1 Introduction","text":"field machine learning sits intersection computer science statistics, core component data science. According Mitchell (1997), “field machine learning concerned question construct computer programs automatically improve experience.”Machine learning approaches divided two main types.Supervised:\ntraining “predictive” model data;\none () attribute dataset used “predict” another attribute.\ntraining “predictive” model data;one () attribute dataset used “predict” another attribute.Unsupervised:\ndiscovery descriptive patterns data;\ncommonly used data mining.\ndiscovery descriptive patterns data;commonly used data mining.Clustering classic unsupervised machine learning task, aims \"automatically divides data clusters , groups similar items\"(Lantz, 2019). computer science, wide range approaches developed tackle clustering. Among approaches, two common centroid-based approaches (k-means) hierarchical approaches. approaches include density-based clustering methods (DBSCAN) mixed approaches (bagged clustering), combine different aspects centroid-based hierarchical approaches.","code":""},{"path":"unsupervised-machine-learning.html","id":"k-means","chapter":"10 Unsupervised machine learning","heading":"10.1.1 K-means","text":"k-mean approach clusters \\(n\\) observations (\\(x\\)) \\(k\\) clusters (\\(c\\)) minimising within-cluster sum squares (WCSS) iterative process. , algorithm calculates distance observation (.e., case, object, row table) centroid cluster. square values distances summed cluster, whole dataset. aim algorithm minimise value.\\[WCSS = \\sum_{c=1}^{k} \\sum_{x \\c} (x - \\overline{x}_c)^2\\]minimise WCSS, trying identify k clusters, k-mean first randomly select k observations initial centroids. , k-means repeats two steps . Every time k-means repeats two steps, new centroids closer two actual centre. process continues centroids don’t change anymore (within certain margin error) reached maximum number iterations set analyst.assignment step: observations assigned closest centroidsupdate step: calculate means cluster, new centroid","code":""},{"path":"unsupervised-machine-learning.html","id":"number-of-clusters","chapter":"10 Unsupervised machine learning","heading":"10.1.2 Number of clusters","text":"key limitation k-mean requires select number clusters identified advance. Unfortunately, analysts always position knowing advance many clusters supposed within data analysing. cases, number heuristics can used select number clusters best fits data.well-known method “elbow method”. approach suggests calculate k-means range values k, calculate WCSS obtained k, select value k minimises WCSS without increasing number clusters beyond point decrease WCSS minimal. approach called “elbow method” (can seen examples ) printing line representing value WCSS values k taken account, suggests select value k “elbow” inflation point line.heuristics exist, suggest using alternative measures cluster quality. instance, cluster library provides simple ways calculate silhouette measure gap statistic. silhouette value indicates well observations fit within clusters, whereas gap statistic measures dispersion within cluster, compared uniform distribution values. higher value gap statistic, away distribution uniform (thus higher quality clustering).three heuristics, best approach calculate values using bootstrapping approach. , calculate statistics multiple times samples dataset, order account random variation. However, clusGap function cluster library allows bootstrapping natively, illustrated .","code":""},{"path":"unsupervised-machine-learning.html","id":"geodemographic-classification","chapter":"10 Unsupervised machine learning","heading":"10.1.3 Geodemographic Classification","text":"GIScience, clustering approaches commonly used create geodemographic classifications. instance, Gale et al., 2016 created 2011 Output Area Classification (2011 OAC) starting initial set 167 prospective variables UK Census 2011.process creating classification, 86 variables removed initial set, including highly correlated variables don’t bring additional information classification process. Furthermore, 41 variable retained , whereas 40 combined, create final set 60 variables. k-mean approach applied cluster census Output Areas (OAs) 8 supergroups, 26 groups 76 subgroups.paper provides detail report process. particular, interesting see authors applied process variable selection involving repeated clustering excluding one variable, see within cluster sum square measure (WCSS) affected. Variable produced significantly higher WCSS excluded considered exclusion final analysis, order increase homogeneity clusters.clustering completed, final step geodemographic classification interpretation resulting cluster, commonly done observing average values variables cluster.","code":""},{"path":"unsupervised-machine-learning.html","id":"examples-1","chapter":"10 Unsupervised machine learning","heading":"10.2 Examples","text":"two examples explore creation simple geodemographic classifications city Leicester, using variables United Kingdom 2011 Census included among 167 initial variables Gale et al., 2016 taken account creating 2011 Output Area Classification.variables going take account five ones listed , plus total count statistical unit, Total_Dwellings.u086: Detachedu087: Semi-detachedu088: Terraced (including end-terrace)u089: Flatsu090: Caravan mobile temporary structureThe code extracts necessary variables original dataset applies normalisation steps across five variables listed . Finally, columns renamed user-friendly names adding perc_ front column name.first step k-means select observations random initial centroids. result, every time run computation, starting point slightly different, might result. particular, likely cluster order might chance (.e., cluster 1 one time might cluster 3 next), although overall result stable. Nevertheless, make document reproducible, can set “seed” generation random numbers. ensure observations selected random , thus results . can done R, using function set.seed providing number (relatively large number ) “seed”.","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n## ✔ tibble  3.1.4     ✔ dplyr   1.0.7\n## ✔ tidyr   1.1.3     ✔ stringr 1.4.0\n## ✔ readr   2.0.1     ✔ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(magrittr)## \n## Attaching package: 'magrittr'## The following object is masked from 'package:purrr':\n## \n##     set_names## The following object is masked from 'package:tidyr':\n## \n##     extract\nlibrary(cluster)\nleicester_2011OAC <- readr::read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\nleicester_dwellings <-\n  leicester_2011OAC %>%\n  dplyr::select(\n    OA11CD, Total_Dwellings,\n    u086:u090\n  ) %>%\n  # scale across\n  dplyr::mutate(\n    dplyr::across( \n      u086:u090,\n      #scale\n      function(x){ (x / Total_Dwellings) * 100 }\n    )\n  ) %>%\n  dplyr::rename(\n    detached = u086,\n    semidetached = u087,\n    terraced = u088,\n    flats = u089,   \n    carava_tmp = u090\n  ) %>%\n  # rename columns\n  dplyr::rename_with(\n    function(x){ paste0(\"perc_\", x) },\n    detached:carava_tmp\n  )\nset.seed(20201208)"},{"path":"unsupervised-machine-learning.html","id":"terrace-and-semi-detached-houses","chapter":"10 Unsupervised machine learning","heading":"10.2.1 Terrace and semi-detached houses","text":"first example explores create geodemographic classification using two variables.u087 (now semidetached): Semi-detachedu088(now terraced): Terraced (including end-terrace)first step, can explore relationship two variables. code illustrates use ggpairs GGally library, provides additional complex plots top ggplot2 framework.scatterplot seems indicate least three clusters might exist data. One lot semi-detached terraced house (top-left corner scatterplot); one lot terraced semi-detached houses (bottom-right corner scatterplot); one classes (bottom-left corner scatterplot).However, clearly many OAs populate area three groups. , best approach cluster OAs?code illustrates create three plots. first follows elbow method heuristic. second third take similar approach using silhouette gap statistic.Based WCSS plot, number clusters k best fitting data range k = 3 k = 6, around inflation point (elbow) line. silhouette plot shows local maximum k = 3 k = 6, indicates observations best fit within clusters 3 6 clusters created. gap statistic steadily increases reaches plateau around k = 5. indicates clustering improves move 2 5 clusters, quality doesn’t increase much afterwards. value k = 6 (value suggested two heuristics) local minimum, still difference neighbouring values relatively small.Overall, heuristics seem indicate k = 3 lead good clustering result. However, gap statistic indicates three clusters compact. probably due fact observations center scatterplot seen rather uniformally distributed space three main clusters. , chosing k = 6 clusters might best fitting approach case. can calculate clusters k = 6 shown .","code":"\n# install.packages(\"GGally\")\nlibrary(GGally)\n\nleicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced) %>%\n  GGally::ggpairs(\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\n# Get only the data necessary for testing\ndata_for_testing <-\n  leicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced)\n\n# Calculate WCSS and silhouette\n# for k = 2 to 15\n# Set up two vectors where to store\n# the calculated WCSS and silhouette value\ntesting_wcss <- rep(NA, 15)\ntesting_silhouette <- rep(NA, 15)\n\n# for k = 2 to 15\nfor (testing_k in 2:15){\n  # Calculate kmeans\n  kmeans_result <- \n    stats::kmeans(data_for_testing, centers = testing_k, iter.max = 50)\n  \n  # Extract WCSS\n  # and save it in the vector\n  testing_wcss[testing_k] <- kmeans_result %$% tot.withinss\n  \n  # Calculate average silhouette\n  # and save it in the vector\n  testing_silhouette[testing_k] <- \n    kmeans_result %$% cluster %>%\n    cluster::silhouette(\n      data_for_testing %>% dist()\n    ) %>%\n    magrittr::extract(, 3) %>% mean()\n}\n\n# Calculate the gap statistic using bootstrapping\ntesting_gap <- \n  cluster::clusGap(\n    data_for_testing, \n    FUN = kmeans, \n    K.max = 15, # max number of clusters\n    B = 50      # number of samples\n  )\n# Plots\nplot(2:15, testing_wcss[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"WCSS\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_silhouette[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Silhouette\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_gap[[\"Tab\"]][2:15, \"gap\"], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Gap\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nterr_sede_kmeans <- leicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced) %>%\n  stats::kmeans(centers = 6, iter.max = 50)\n\nleicester_dwellings <- \n  leicester_dwellings %>%\n  tibble::add_column(\n    terr_sede_cluster = terr_sede_kmeans %$% cluster %>% as.character()\n  )"},{"path":"unsupervised-machine-learning.html","id":"interpreting-the-clusters","chapter":"10 Unsupervised machine learning","heading":"10.2.2 Interpreting the clusters","text":"clustering completed, can analyse results visual analysis. instance, can plot two original variables, along computed clusters illustrated .Another common approach interpreting results create heatmaps average values variables used clustering process cluster.plot , clearly illustrates cluster 6 high percentage semi-detached houses low percentages terraced houses. Cluster 2 high percentage terraced houses low percentages semi-detached houses. Cluster 4 low percentage semi-detached terraced houses. three clusters first identified first scatterplot .Moreover, clustering process identifies cluster 1, includes similar percentages semi-detached terraced houses; well cluster 5, including mostly semi-detached also terraced houses, cluster 3, including mostly terraced also semi-detached houses.","code":"\nleicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced, terr_sede_cluster) %>%\n  GGally::ggpairs(\n    mapping = aes(color = terr_sede_cluster),\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\nleicester_dwellings %>%\n  group_by(terr_sede_cluster) %>%\n  dplyr::summarise(\n    avg_perc_semidetached = mean(perc_semidetached), \n    avg_perc_terraced = mean(perc_terraced)\n  ) %>%\n  dplyr::select(terr_sede_cluster, avg_perc_semidetached, avg_perc_terraced) %>%\n  tidyr::pivot_longer(\n    cols = -terr_sede_cluster,\n    names_to = \"clustering_dimension\",\n    values_to = \"value\"\n  ) %>%\n  ggplot2::ggplot(\n    aes(\n      x = clustering_dimension,\n      y = terr_sede_cluster\n    )\n  ) +\n  ggplot2::geom_tile(aes(fill = value)) +\n  ggplot2::xlab(\"Clustering dimension\") + \n  ggplot2::ylab(\"Cluster\") +\n  ggplot2::scale_fill_viridis_c(option = \"inferno\") +\n  ggplot2::theme_bw()"},{"path":"unsupervised-machine-learning.html","id":"a-geodemographic-of-dwelling-types","chapter":"10 Unsupervised machine learning","heading":"10.2.3 A geodemographic of dwelling types","text":"case study useful simple example create geodemographic classification, possibly interesting analysis due limited number variables used. section, explore creation geodemographic dwelling types city Leicester, using five variables available original dataset.can start visual analysis data. relationship different variables generally resembles one seen semi-detached terraced houses seen example , except caravans mobile temporary structures, seem fairly rare Leicester. variable seems particularly well correlated . Thus variables included classification, potential contribute relevant information.order identify number clusters k best fits data, can use elbow method, along silhouette gap statistic measures, seen previous example. difference case data_for_testing include five variables.previous example, elbow method (.e., WCSS), silhouette gap statistic seem indicate k = 3 k = 6 might best choice. Let’s see result choosing k = 6.","code":"\nleicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp) %>%\n  GGally::ggpairs(\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\n# Data for elbow method\ndata_for_testing <-\n  leicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp)\n\n# Calculate WCSS and silhouette\n# for k = 2 to 15\n# Set up two vectors where to store\n# the calculated WCSS and silhouette value\ntesting_wcss <- rep(NA, 15)\ntesting_silhouette <- rep(NA, 15)\n\n# for k = 2 to 15\nfor (testing_k in 2:15){\n  # Calculate kmeans\n  kmeans_result <- \n    stats::kmeans(data_for_testing, centers = testing_k, iter.max = 50)\n  \n  # Extract WCSS\n  # and save it in the vector\n  testing_wcss[testing_k] <- kmeans_result %$% tot.withinss\n  \n  # Calculate average silhouette\n  # and save it in the vector\n  testing_silhouette[testing_k] <- \n    kmeans_result %$% cluster %>%\n    cluster::silhouette(\n      data_for_testing %>% dist()\n    ) %>%\n    magrittr::extract(, 3) %>% mean()\n}\n\n# Calculate the gap statistic using bootstrapping\ntesting_gap <- \n  cluster::clusGap(data_for_testing, FUN = kmeans, \n    K.max = 15, B = 50\n  )\n# Plots\nplot(2:15, testing_wcss[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"WCSS\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_silhouette[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Silhouette\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_gap[[\"Tab\"]][2:15, \"gap\"], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Gap\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\ndwellings_kmeans <- leicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp) %>%\n  stats::kmeans(\n    centers = 6, \n    iter.max = 50\n  )\n\nleicester_dwellings <- \n  leicester_dwellings %>%\n  tibble::add_column(\n    dwellings_cluster = \n      dwellings_kmeans %$% \n        cluster %>%\n          as.character()\n  )"},{"path":"unsupervised-machine-learning.html","id":"interpreting-the-clusters-1","chapter":"10 Unsupervised machine learning","heading":"10.2.4 Interpreting the clusters","text":"first exploratory plot clusters seems reveal clusters closely resemble seen first example .previous example, can use “heatmap” plot explore clusters characterised variables used clustering process (see also Exercise 414.1.1 ).Another common approach explore characteristics clusters created k-means geodemongraphic classification use radar charts (also known spider charts, web charts polar charts), can created R using number libraries, including radarchart fmsb library.radar charts effective visualising values multiple varaibles, long variables similar type, value range. case, values percentages, radar chart effective illustrating variables particularly high averages cluster.Finally, can map cluster cartographically analyse spatial distribution.","code":"\nleicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp, dwellings_cluster) %>%\n  GGally::ggpairs(\n    mapping = aes(color = dwellings_cluster),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\ndwellings_cluster_avgs <-\n  leicester_dwellings %>%\n  group_by(dwellings_cluster) %>%\n  dplyr::summarise(\n    dplyr::across(\n      perc_detached:perc_carava_tmp,\n      mean\n    ) \n  ) %>%\n  # rename columns\n  dplyr::rename_with(\n    function(x){ paste0(\"avg_\", x) },\n    perc_detached:perc_carava_tmp\n  )\n  \ndwellings_cluster_avgs %>%\n  tidyr::pivot_longer(\n    cols = -dwellings_cluster,\n    names_to = \"clustering_dimension\",\n    values_to = \"value\"\n  )  %>%\n  ggplot2::ggplot(\n    aes(\n      x = clustering_dimension,\n      y = dwellings_cluster\n    )\n  ) +\n  ggplot2::geom_tile(\n    aes(\n      fill = value\n    )\n  ) +\n  ggplot2::xlab(\"Clustering dimension\") + \n  ggplot2::ylab(\"Cluster\") +\n  ggplot2::scale_fill_viridis_c(option = \"inferno\") +\n  ggplot2::theme_bw() +\n  ggplot2::theme(\n    axis.text.x = \n      element_text(\n        angle = 90, \n        vjust = 0.5, \n        hjust=1\n      )\n    )\n# install.packages(\"fmsb\")\nlibrary(fmsb)\n\npar(mar=rep(3,4))\npar(mfrow=c(3,2))\n\nfor(cluster_number in 1:6){\n  rbind (\n    # The radar chart requires a maximum and a minimum row \n    # before the actual data\n    rep(100, 5), # max 100% for 5 variables\n    rep(0, 5),   # min 0% for 5 variables\n    dwellings_cluster_avgs %>%\n      dplyr::filter(dwellings_cluster == cluster_number) %>%\n      dplyr::select(-dwellings_cluster) %>%\n      as.data.frame()\n    ) %>%\n    fmsb::radarchart(title = paste(\"Cluster\", cluster_number))\n}"},{"path":"unsupervised-machine-learning.html","id":"exercise-414.1","chapter":"10 Unsupervised machine learning","heading":"10.3 Exercise 414.1","text":"Question 414.1.1: Based “heatmap”, radar charts map created example , characterise five clusters? name ?.Question 414.1.2: Create geodemographic classification using data seen second example , creating k = 9 clusters.Question 414.1.3: Create geodemographic classification city Leicester based presence peoples different age groups included 2011_OAC_Raw_uVariables_Leicester.csv dataset (u007 u019).Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""}]
