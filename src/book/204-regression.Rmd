# Regression analysis

<br/><small>*This chapter is currently a draft.*</small>

<br/><small><a href="javascript:if(window.print)window.print()">Print this chapter</a></small>


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
```

This chapter focuses on regression analysis, starting from simple regression and then multiple regression analysis. Both sections discuss how to run a regression analysis and (importantly) how to check that the analysis meets the test assumptions and how to report the results in an analysis document.



## Simple regression

The simple regression analysis is a supervised machine learning approach to creating a model able to predict the value of one outcome variable $Y$ based on one predictor variable $X_1$, by estimating the intercept $b_0$ and coefficient (slope) $b_1$, and accounting for a reasonable amount of error $\epsilon$. 

$$Y_i = (b_0 + b_1 * X_{i1}) + \epsilon_i $$

Least squares is the most commonly used approach to generate a regression model. This model fits a line to minimise the squared values of the **residuals** (errors), which are calculated as the squared difference between observed values the values predicted by the model. 

$$redidual = \sum(observed - model)^2$$

A model is considered **robust** if the residuals do not show particular trends, which would indicate that *"something"* is interfering with the model. In particular, the assumption of the regression model are:

- **linearity:** the relationship is actually linear;
- **normality** of residuals: standard residuals are normally distributed with mean `0`;
- **homoscedasticity** of residuals: at each level of the predictor variable(s) the variance of the standard residuals should be the same (*homo-scedasticity*) rather than different (*hetero-scedasticity*);
- **independence** of residuals: adjacent standard residuals are not correlated.

### Simple regression example

The example that we have seen in the lecture illustrated how simple regression can be used to create a model to predict the arrival delay based on the departure delay of a flight, based on the data available in the `nycflights13` dataset for the flight on November 20th, 2013. 

$$arr\_delay_i = (Intercept + Coefficient_{dep\_delay} * dep\_delay_{i1}) + \epsilon_i $$

The last line below arranges the table by arrival delay. It is advisable to give a meaningful order to the data (e.g., based on the outcome) before creating the model, in order to facilitate a robust execution of the Durbin-Watson test (see below).

```{r, echo=TRUE}
# Load the library
library(nycflights13)

# November 20th, 2013
flights_nov_20 <- flights %>%
  filter(
    !is.na(dep_delay) &
    !is.na(arr_delay) &
    month == 11 &
    day ==20
  ) %>% 
  arrange(arr_delay)
```

The scatterplot below seems to indicate that the relationship is indeed linear.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
flights_nov_20 %>%
  ggplot(
    aes(
      x = dep_delay, 
      y = arr_delay
    )
  ) +
  geom_point() + 
  coord_fixed(ratio = 1) +
  theme_bw()
```

The code below generates the model using the function `lm`, and the function `summary` to obtain the summary of the results of the test. The model and summary are saved in the variables `delay_model` and `delay_model_summary`, respectively, for further use below. The variable `delay_model_summary` can then be called directly to visualise the result of the test.

```{r, echo=TRUE}
# Classic R coding version
# delay_model <- lm(arr_delay ~ dep_delay, data = flights_nov_20)
# delay_model_summary <- summary(delay_model)

# Load magrittr library to use %$%
library(magrittr)

delay_model <- flights_nov_20 %$%
  lm(arr_delay ~ dep_delay) 

delay_model_summary <- delay_model %>%
  summary()

delay_model_summary
```

The image below highlights the important values in the output: the adjusted $R^2$ value; the model significance value `p-value` and the related F-statistic information `F-statistic`; the intercept and `dep_delay` coefficient estimates in the `Estimate` column and the related significance values of in the column `Pr(>|t|)`.


![](images/Regression_output_annotated.png){width=70%}


The output indicates:

- **p-value: < 2.2e-16**: $p<.001$ the model is significant;
    - derived by comparing the calulated **F-statistic** value to F distribution `r delay_model_summary$fstatistic[1] %>% round(digits = 2)` having specified degrees of freedom (`r delay_model_summary$fstatistic[2]`, `r delay_model_summary$fstatistic[3]`);
    - Report as: $F(`r delay_model_summary[["fstatistic"]][2]`, `r delay_model_summary[["fstatistic"]][3]`) = `r delay_model_summary[["fstatistic"]][1] %>% round(digits = 2)`$
- **Adjusted R-squared: `r delay_model_summary$adj.r.squared %>% round(digits = 4)`**: the departure delay can account for `r (delay_model_summary$adj.r.squared * 100) %>% round(digits = 2)`% of the arrival delay;
- **Coefficients**:
    - Intercept estimate `r delay_model_summary$coefficients[1,1] %>% round(digits = 4)` is significant;
    - `dep_delay` coefficient (slope) estimate `r delay_model_summary$coefficients[2,1] %>% round(digits = 4)` is significant.


```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay, y = arr_delay)) +
  geom_point() + coord_fixed(ratio = 1) +
  geom_abline(intercept = 4.0943, slope = 1.04229, color="red")
```


### Checking regression assumptions

#### Normality

The Shapiro-Wilk test can be used to check for the normality of standard residuals. The test should be not significant for robust models. In the example below, the standard residuals are *not* normally distributed. However, the plot further below does show that the distribution of the residuals is not far away from a normal distribution.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
delay_model %>% 
  rstandard() %>% 
  shapiro.test()
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
delay_model %>% 
  rstandard() %>%
  data.frame(std_res = .) %>%
  ggplot(aes(x = std_res)) +
  geom_histogram(
    aes(
      y =..density..
    ),
    bins = 100
  ) + 
  stat_function(
    fun = dnorm, 
    args = list(
      mean = delay_model %>% rstandard() %>% mean(),
      sd = delay_model %>% rstandard() %>% sd()),
    colour = "red", size = 1)
```


#### Homoscedasticity

The Breusch-Pagan test can be used to check for the homoscedasticity of standard residuals. The test should be not significant for robust models. In the example below, the standard residuals are homoscedastic.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(lmtest)

delay_model %>% 
  bptest()
```

#### Independence

The Durbin-Watson test can be used to check for the independence of residuals. The test should be statistic should be close to 2 (between 1 and 3) and not significant for robust models. In the example below, the standard residuals might not be completely independent. Note, however, that the result depends on the order of the data.

```{r, echo=TRUE}
# Also part of the library lmtest
delay_model %>%
  dwtest()
```

The idea of autocorrelation of the residuals tested by the Durbin-Watson test can be illustrated using the lag plot below, where the standard residual of a case is compared to the standard residual of the previous case in the table. A clear patter in the plot would indicate that the residuals are not independent. In the plot below, no clear pattern is visible, thus reinforcing the results of the test above.

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
delay_model %>%
  rstandard() %>% 
  lag.plot()
```


#### Plots

The [`plot.lm` function](https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/plot.lm) can be used to further explore the residuals visuallly. Usage is illustrated below. The *Residuals vs Fitted* and *Scale-Location* plot provide an insight into the homoscedasticity of the residuals, the *Normal Q-Q* plot provides an illustration of the normality of the residuals, and the *Residuals vs Leverage* can be useful to identify exceptional cases (e.g., Cook's distance greater than 1).

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 7}
delay_model %>%
  plot(which = c(1))

delay_model %>%
  plot(which = c(2))

delay_model %>%
  plot(which = c(3))

delay_model %>%
  plot(which = c(5))
```


### How to report a simple regression

Overall, we can say that the delay model computed above is fit ($F(`r delay_model_summary[["fstatistic"]][2]`, `r delay_model_summary[["fstatistic"]][3]`) = `r delay_model_summary[["fstatistic"]][1] %>% round(digits = 2)`$, $p < .001$), indicating that the departure delay might account for `r (delay_model_summary$adj.r.squared * 100) %>% round(digits = 2)`% of the arrival delay. However the model is only partially robust. The residuals satisfy the homoscedasticity assumption (Breusch-Pagan test, $BP = `r bptest(delay_model)[["statistic"]] %>% round(digits = 2)`$, $p =`r bptest(delay_model)[["p.value"]] %>% round(digits = 2)`$), and the independence assumption (Durbin-Watson test, $DW = `r dwtest(delay_model)[["statistic"]] %>% round(digits = 2)`$, $p =`r dwtest(delay_model)[["p.value"]] %>% round(digits = 2)`$), but they are not normally distributed (Shapiro-Wilk test, $W =  `r shapiro.test(rstandard(delay_model))[["statistic"]] %>% round(digits = 2)`$, $p < .001$).

The [`stargazer` function of the `stargazer` library](https://www.rdocumentation.org/packages/stargazer/versions/5.2.2/topics/stargazer) can be applied to the model `delay_model` to generate a nicer output in RMarkdown PDF documents by including `results = "asis"` in the R snippet option.

```{r, echo=TRUE, results = "asis", message=FALSE, warning=FALSE}
# Install stargazer if not yet installed
# install.packages("stargazer")

library(stargazer)

# Not rendered in bookdown
stargazer(delay_model, header = FALSE)
```


## Exercise 204.1

**Question 204.1.1:** While linear regression modelling does not require the variables to be normally distributed, very skewed variables such as `dep_delay` and `arr_delay` might be a significant obstacle to a robust model. Is it possible to build a robust model using *"un-skewed"* variables?


## Multiple regression

The multiple regression analysis is a supervised machine learning approach to creating a model able to predict the value of one outcome variable $Y$ based on two or more predictor variables $X_1 \dots X_M$, by estimating the intercept $b_0$ and the coefficients (slopes) $b_1 \dots b_M$, and accounting for a reasonable amount of error $\epsilon$. 

$$Y_i = (b_0 + b_1 * X_{i1} + b_2 * X_{i2} + \dots + b_M * X_{iM}) + \epsilon_i $$

The assumptions are the same as the simple regression, plus the assumption of **no multicollinearity**: if two or more predictor variables are used in the model, each pair of variables not correlated. This assumption can be tested by checking the variance inflation factor (VIF). If the largest VIF value is greater than 10 or the average VIF is substantially greater than 1, there might be an issue of multicollinearity.

### Multiple regression example

The example below explores whether a regression model can be created to estimate the number of people in Leicester commuting to work using private transport (`u121`) in Leicester, using the number of people in different industry sectors as predictors. 

For instance, occupations such as electricity, gas, steam and air conditioning supply (`u144`) require to travel some distances with equipment, thus the related variable `u144` is included in the model, whereas people working in information and communication might be more likely to work from home or commute by public transport. 

```{r, echo=TRUE, eval=FALSE}
leicester_2011OAC <- read_csv("2011_OAC_Raw_uVariables_Leicester.csv")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
leicester_2011OAC <- read_csv(paste0(Sys.getenv("RGDS_HOME"), "/data/", "2011_OAC_Raw_uVariables_Leicester.csv"))
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Select and 
# normalise variables
leicester_2011OAC_transp <-
  leicester_2011OAC %>%
  select(
    OA11CD, 
    Total_Pop_No_NI_Students_16_to_74, Total_Employment_16_to_74, 
    u121, u141:u158
  ) %>%
  # percentage method of travel
  mutate(
    u121 = (u121 / Total_Pop_No_NI_Students_16_to_74) * 100
  ) %>%
  # percentage across industry sector columns
  mutate(
    across( 
      u141:u158,
      function(x){ (x / Total_Employment_16_to_74) * 100 }
    )
  ) %>%
  # rename columns
  rename_with(
    function(x){ paste0("perc_", x) },
    c(u121, u141:u158)
  ) %>% 
  arrange(perc_u121)
```

Let's observe how all those variable relate to one another using a pairs plot.


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 16, fig.height = 16}
library(GGally)

leicester_2011OAC_transp %>%
  select(perc_u121, perc_u141:perc_u158) %>%
  ggpairs(
    upper = list(continuous = 
        wrap(ggally_cor, method = "kendall")),
    lower = list(continuous = 
        wrap("points", alpha = 0.3, size=0.1))
  ) +
  theme_bw()
```

Based on the plot above and our understanding of the variables, we can try create a model able to relate and estimate the dependent (output) variable `perc_u120` (*Method of Travel to Work, Private Transport*) with the independent (predictor) variables:

- `perc_u142`: Industry Sector, Mining and quarrying
- `perc_u144`: Industry Sector, Electricity, gas, steam and air conditioning ...
- `perc_u146`: Industry Sector, Construction
- `perc_u149`: Industry Sector, Accommodation and food service activities

A multiple regression model can be specified in a similar way as a simple regression model, using the same `lm` function, but adding the additional predictor variables using a `+` operator.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Create model
commuting_model1 <- 
  leicester_2011OAC_transp %$%
  lm(
    perc_u121 ~ 
      perc_u142 + perc_u144 + perc_u146 + perc_u149
  )

# Print summary
commuting_model1 %>%
  summary()
```

```{r, echo=TRUE, results = "asis", message=FALSE, warning=FALSE}
# Not rendered in bookdown
stargazer(commuting_model1, header=FALSE)
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
commuting_model1 %>%
  rstandard() %>% 
  shapiro.test()
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
commuting_model1 %>% 
  bptest()
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
commuting_model1 %>%
  dwtest()
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(car)

commuting_model1 %>%
  vif()
```

The output above suggests that the model is fit ($F(`r summary(commuting_model1)[["fstatistic"]][2]`, `r summary(commuting_model1)[["fstatistic"]][3]`) = `r summary(commuting_model1)[["fstatistic"]][1] %>% round(digits = 2)`$, $p < .001$), indicating that a model based on the presence of people working in the four selected industry sectors can account for `r (summary(commuting_model1)$adj.r.squared * 100) %>% round(digits = 2)`% of the number of people using private transportation to commute to work. However the model is only partially robust. The residuals are normally distributed (Shapiro-Wilk test, $W =  `r shapiro.test(rstandard(commuting_model1))[["statistic"]] %>% round(digits = 2)`$, $p =`r shapiro.test(rstandard(commuting_model1))[["p.value"]] %>% round(digits = 2)`$) and there seems to be no multicollinearity with average VIF $`r commuting_model1 %>% vif() %>% mean() %>% round(digits = 2)`$, but the residuals don't satisfy the homoscedasticity assumption (Breusch-Pagan test, $BP = `r bptest(commuting_model1)[["statistic"]] %>% round(digits = 2)`$, $p < .001$), nor the independence assumption (Durbin-Watson test, $DW = `r dwtest(commuting_model1)[["statistic"]] %>% round(digits = 2)`$, $p < .01$).

The coefficient values calculated by the `lm` functions are important to create the model, and provide useful information. For instance, the coefficient for the variable `perc_u144` is `1.169`, which indicates that if the presence of people working in electricity, gas, steam and air conditioning supply increases by one percentage point, the number of people using private transportation to commute to work increases by `1.169` percentage points, according to the model. The coefficients also indicate that the presence of people working in accommodation and food service activities actually has a negative impact (in the context of the variables selected for the model) on the number of people using private transportation to commute to work.

In this example, all variables use the same unit and are of a similar type, which makes interpreting the model relatively simple. When that is not the case, it can be useful to look at the standardized $\beta$, which provide the same information but measured in terms of standard deviation, which make comparisons between variables of different types easier to draw. For instance, the values calculated below using the function `lm.beta` of the library `lm.beta` indicate that if the presence of people working in construction has the highest impact on the outcome varaible. If the presence of people working in construction increases by one standard deviation, the number of people using private transportation to commute to work increases by `0.29` standard deviations, according to the model.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Install lm.beta library if necessary
# install.packages("lm.beta")
library(lm.beta)

lm.beta(commuting_model1)
```

## Exercise 204.2

**Question 204.2.1:** Is it possible to create a model linking population density to housing type in Leicester?


---

<small>by [Stefano De Sabbata](https://sdesabbata.github.io/) -- text licensed under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), contains public sector information licensed under the [Open Government Licence v3.0](http://www.nationalarchives.gov.uk/doc/open-government-licence), code licensed under the [GNU GPL v3.0](https://www.gnu.org/licenses/gpl-3.0.html).</small>